{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FBT Entertainment Expense Classification Pipeline\n",
    "## Rabobank Australia - FBT Year Ended 31 March 2025\n",
    "\n",
    "### Pipeline Features:\n",
    "- Comprehensive data cleaning and preprocessing\n",
    "- Location extraction and geocoding\n",
    "- Travel distance and time estimation\n",
    "- Advanced text feature engineering\n",
    "- Supervised classification using ME? Y/N labels\n",
    "- Optional unsupervised clustering for pattern discovery\n",
    "- Multiple model comparison with hyperparameter tuning\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-05T08:21:15.234735Z",
     "start_time": "2025-12-05T08:21:15.225718Z"
    }
   },
   "source": [
    "# =============================================================================\n",
    "# CONFIGURATION\n",
    "# =============================================================================\n",
    "\n",
    "CONFIG = {\n",
    "    # File paths\n",
    "    'raw_data_path': 'data/data_raw_2024-25.xlsx',\n",
    "    'wp_files': [\n",
    "        'data/WP_1_Apr_to_Dec_24_FBT_ent_acc.xlsx',\n",
    "        'data/WP_2_Apr_to_Dec_24_FBT_ent_acc.xlsx',\n",
    "        'data/WP_3_Jan_to_Mar_25_FBT_ent_acc.xlsx',\n",
    "        'data/WP_4_Jan_to_Mar_25_FBT_ent_acc.xlsx'\n",
    "    ],\n",
    "    \n",
    "    # Reference location for distance calculations (Sydney CBD - Rabobank office)\n",
    "    'reference_location': {'lat': -33.8688, 'lon': 151.2093, 'name': 'Sydney CBD'},\n",
    "    \n",
    "    # Classification mode: 'supervised' or 'unsupervised'\n",
    "    'mode': 'supervised',\n",
    "    \n",
    "    # Enable/disable features\n",
    "    'enable_geocoding': True,\n",
    "    'enable_clustering': True,\n",
    "    \n",
    "    # Clustering settings\n",
    "    'n_clusters': 8,\n",
    "    \n",
    "    # Model training\n",
    "    'test_size': 0.2,\n",
    "    'random_state': 42,\n",
    "    'cv_folds': 5,\n",
    "    \n",
    "    # Text vectorization\n",
    "    'max_features': 5000,\n",
    "    'ngram_range': (1, 3),\n",
    "    'min_df': 2,\n",
    "    'max_df': 0.95,\n",
    "    \n",
    "    # Output paths\n",
    "    'model_output': 'fbt_classifier_pipeline.joblib',\n",
    "    'encoder_output': 'fbt_label_encoder.joblib',\n",
    "    'predictions_output': 'fbt_predictions.csv'\n",
    "}"
   ],
   "outputs": [],
   "execution_count": 14
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Imports and Setup"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-05T08:21:16.819022Z",
     "start_time": "2025-12-05T08:21:16.712954Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import warnings\n",
    "from collections import Counter\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "import math\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "plt.rcParams['font.size'] = 10\n",
    "\n",
    "# Text processing\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "\n",
    "# Preprocessing\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, MinMaxScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Model selection\n",
    "from sklearn.model_selection import (\n",
    "    train_test_split, cross_val_score, StratifiedKFold,\n",
    "    GridSearchCV, RandomizedSearchCV, learning_curve\n",
    ")\n",
    "\n",
    "# Supervised models\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier, RidgeClassifier\n",
    "from sklearn.ensemble import (\n",
    "    RandomForestClassifier, GradientBoostingClassifier,\n",
    "    AdaBoostClassifier, ExtraTreesClassifier, VotingClassifier,\n",
    "    BaggingClassifier, StackingClassifier\n",
    ")\n",
    "from sklearn.naive_bayes import MultinomialNB, ComplementNB\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "\n",
    "# Unsupervised models\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering, DBSCAN\n",
    "from sklearn.decomposition import PCA, TruncatedSVD, LatentDirichletAllocation, NMF\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# Evaluation\n",
    "from sklearn.metrics import (\n",
    "    classification_report, confusion_matrix, accuracy_score,\n",
    "    precision_recall_fscore_support, roc_auc_score, f1_score,\n",
    "    ConfusionMatrixDisplay, roc_curve, precision_recall_curve,\n",
    "    silhouette_score, calinski_harabasz_score, davies_bouldin_score\n",
    ")\n",
    "\n",
    "# Pipeline\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "# Imbalanced learning\n",
    "try:\n",
    "    from imblearn.over_sampling import SMOTE, ADASYN, RandomOverSampler\n",
    "    from imblearn.under_sampling import RandomUnderSampler, TomekLinks\n",
    "    from imblearn.combine import SMOTETomek\n",
    "    from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "    IMBLEARN_AVAILABLE = True\n",
    "except ImportError:\n",
    "    IMBLEARN_AVAILABLE = False\n",
    "    print(\"Note: imbalanced-learn not installed. Run: pip install imbalanced-learn\")\n",
    "\n",
    "# Geocoding\n",
    "try:\n",
    "    from geopy.geocoders import Nominatim\n",
    "    from geopy.distance import geodesic\n",
    "    from geopy.exc import GeocoderTimedOut\n",
    "    GEOPY_AVAILABLE = True\n",
    "except ImportError:\n",
    "    GEOPY_AVAILABLE = False\n",
    "    print(\"Note: geopy not installed. Run: pip install geopy\")\n",
    "\n",
    "# Model persistence\n",
    "import joblib\n",
    "\n",
    "print(\"All imports successful\")\n",
    "print(f\"imbalanced-learn available: {IMBLEARN_AVAILABLE}\")\n",
    "print(f\"geopy available: {GEOPY_AVAILABLE}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All imports successful\n",
      "imbalanced-learn available: True\n",
      "geopy available: True\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Australian Location Database"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-05T08:21:18.244221Z",
     "start_time": "2025-12-05T08:21:18.125767Z"
    }
   },
   "source": [
    "# =============================================================================\n",
    "# AUSTRALIAN LOCATIONS DATABASE\n",
    "# Pre-defined coordinates to avoid excessive API calls\n",
    "# =============================================================================\n",
    "\n",
    "AUSTRALIAN_LOCATIONS = {\n",
    "    # Major Cities\n",
    "    'sydney': {'lat': -33.8688, 'lon': 151.2093, 'state': 'NSW', 'type': 'city'},\n",
    "    'melbourne': {'lat': -37.8136, 'lon': 144.9631, 'state': 'VIC', 'type': 'city'},\n",
    "    'brisbane': {'lat': -27.4698, 'lon': 153.0251, 'state': 'QLD', 'type': 'city'},\n",
    "    'perth': {'lat': -31.9505, 'lon': 115.8605, 'state': 'WA', 'type': 'city'},\n",
    "    'adelaide': {'lat': -34.9285, 'lon': 138.6007, 'state': 'SA', 'type': 'city'},\n",
    "    'hobart': {'lat': -42.8821, 'lon': 147.3272, 'state': 'TAS', 'type': 'city'},\n",
    "    'darwin': {'lat': -12.4634, 'lon': 130.8456, 'state': 'NT', 'type': 'city'},\n",
    "    'canberra': {'lat': -35.2809, 'lon': 149.1300, 'state': 'ACT', 'type': 'city'},\n",
    "    \n",
    "    # Regional NSW\n",
    "    'newcastle': {'lat': -32.9283, 'lon': 151.7817, 'state': 'NSW', 'type': 'regional'},\n",
    "    'wollongong': {'lat': -34.4278, 'lon': 150.8931, 'state': 'NSW', 'type': 'regional'},\n",
    "    'dubbo': {'lat': -32.2569, 'lon': 148.6011, 'state': 'NSW', 'type': 'regional'},\n",
    "    'tamworth': {'lat': -31.0830, 'lon': 150.9170, 'state': 'NSW', 'type': 'regional'},\n",
    "    'wagga wagga': {'lat': -35.1082, 'lon': 147.3598, 'state': 'NSW', 'type': 'regional'},\n",
    "    'wagga': {'lat': -35.1082, 'lon': 147.3598, 'state': 'NSW', 'type': 'regional'},\n",
    "    'orange': {'lat': -33.2840, 'lon': 149.1004, 'state': 'NSW', 'type': 'regional'},\n",
    "    'bathurst': {'lat': -33.4190, 'lon': 149.5778, 'state': 'NSW', 'type': 'regional'},\n",
    "    'albury': {'lat': -36.0737, 'lon': 146.9135, 'state': 'NSW', 'type': 'regional'},\n",
    "    'broken hill': {'lat': -31.9539, 'lon': 141.4428, 'state': 'NSW', 'type': 'regional'},\n",
    "    'armidale': {'lat': -30.5150, 'lon': 151.6500, 'state': 'NSW', 'type': 'regional'},\n",
    "    'lismore': {'lat': -28.8120, 'lon': 153.2790, 'state': 'NSW', 'type': 'regional'},\n",
    "    'grafton': {'lat': -29.6850, 'lon': 152.9330, 'state': 'NSW', 'type': 'regional'},\n",
    "    'coffs harbour': {'lat': -30.2963, 'lon': 153.1135, 'state': 'NSW', 'type': 'regional'},\n",
    "    'port macquarie': {'lat': -31.4333, 'lon': 152.9000, 'state': 'NSW', 'type': 'regional'},\n",
    "    'moree': {'lat': -29.4640, 'lon': 149.8470, 'state': 'NSW', 'type': 'regional'},\n",
    "    'goulburn': {'lat': -34.7547, 'lon': 149.7186, 'state': 'NSW', 'type': 'regional'},\n",
    "    'nowra': {'lat': -34.8800, 'lon': 150.6000, 'state': 'NSW', 'type': 'regional'},\n",
    "    'griffith': {'lat': -34.2890, 'lon': 146.0400, 'state': 'NSW', 'type': 'regional'},\n",
    "    'parkes': {'lat': -33.1370, 'lon': 148.1750, 'state': 'NSW', 'type': 'regional'},\n",
    "    'forbes': {'lat': -33.3850, 'lon': 148.0170, 'state': 'NSW', 'type': 'regional'},\n",
    "    'cowra': {'lat': -33.8330, 'lon': 148.6830, 'state': 'NSW', 'type': 'regional'},\n",
    "    'young': {'lat': -34.3130, 'lon': 148.3000, 'state': 'NSW', 'type': 'regional'},\n",
    "    'mudgee': {'lat': -32.6000, 'lon': 149.5830, 'state': 'NSW', 'type': 'regional'},\n",
    "    'cobar': {'lat': -31.4950, 'lon': 145.8380, 'state': 'NSW', 'type': 'regional'},\n",
    "    'bourke': {'lat': -30.0900, 'lon': 145.9380, 'state': 'NSW', 'type': 'regional'},\n",
    "    'walgett': {'lat': -30.0200, 'lon': 148.1170, 'state': 'NSW', 'type': 'regional'},\n",
    "    'narrabri': {'lat': -30.3250, 'lon': 149.7830, 'state': 'NSW', 'type': 'regional'},\n",
    "    'gunnedah': {'lat': -30.9800, 'lon': 150.2500, 'state': 'NSW', 'type': 'regional'},\n",
    "    'inverell': {'lat': -29.7750, 'lon': 151.1170, 'state': 'NSW', 'type': 'regional'},\n",
    "    'glen innes': {'lat': -29.7330, 'lon': 151.7330, 'state': 'NSW', 'type': 'regional'},\n",
    "    'tenterfield': {'lat': -29.0500, 'lon': 152.0170, 'state': 'NSW', 'type': 'regional'},\n",
    "    \n",
    "    # Regional QLD\n",
    "    'gold coast': {'lat': -28.0167, 'lon': 153.4000, 'state': 'QLD', 'type': 'regional'},\n",
    "    'sunshine coast': {'lat': -26.6500, 'lon': 153.0667, 'state': 'QLD', 'type': 'regional'},\n",
    "    'cairns': {'lat': -16.9186, 'lon': 145.7781, 'state': 'QLD', 'type': 'regional'},\n",
    "    'townsville': {'lat': -19.2590, 'lon': 146.8169, 'state': 'QLD', 'type': 'regional'},\n",
    "    'toowoomba': {'lat': -27.5598, 'lon': 151.9507, 'state': 'QLD', 'type': 'regional'},\n",
    "    'rockhampton': {'lat': -23.3791, 'lon': 150.5100, 'state': 'QLD', 'type': 'regional'},\n",
    "    'mackay': {'lat': -21.1411, 'lon': 149.1861, 'state': 'QLD', 'type': 'regional'},\n",
    "    'bundaberg': {'lat': -24.8661, 'lon': 152.3489, 'state': 'QLD', 'type': 'regional'},\n",
    "    'gladstone': {'lat': -23.8427, 'lon': 151.2555, 'state': 'QLD', 'type': 'regional'},\n",
    "    'hervey bay': {'lat': -25.2900, 'lon': 152.8500, 'state': 'QLD', 'type': 'regional'},\n",
    "    'roma': {'lat': -26.5700, 'lon': 148.7850, 'state': 'QLD', 'type': 'regional'},\n",
    "    'charleville': {'lat': -26.4030, 'lon': 146.2430, 'state': 'QLD', 'type': 'regional'},\n",
    "    'longreach': {'lat': -23.4420, 'lon': 144.2500, 'state': 'QLD', 'type': 'regional'},\n",
    "    'mount isa': {'lat': -20.7256, 'lon': 139.4927, 'state': 'QLD', 'type': 'regional'},\n",
    "    'cloncurry': {'lat': -20.7050, 'lon': 140.5060, 'state': 'QLD', 'type': 'regional'},\n",
    "    'emerald': {'lat': -23.5270, 'lon': 148.1640, 'state': 'QLD', 'type': 'regional'},\n",
    "    'dalby': {'lat': -27.1810, 'lon': 151.2650, 'state': 'QLD', 'type': 'regional'},\n",
    "    'kingaroy': {'lat': -26.5400, 'lon': 151.8400, 'state': 'QLD', 'type': 'regional'},\n",
    "    'warwick': {'lat': -28.2150, 'lon': 152.0350, 'state': 'QLD', 'type': 'regional'},\n",
    "    'stanthorpe': {'lat': -28.6570, 'lon': 151.9350, 'state': 'QLD', 'type': 'regional'},\n",
    "    'goondiwindi': {'lat': -28.5470, 'lon': 150.3100, 'state': 'QLD', 'type': 'regional'},\n",
    "    'st george': {'lat': -28.0370, 'lon': 148.5830, 'state': 'QLD', 'type': 'regional'},\n",
    "    'cunnamulla': {'lat': -28.0670, 'lon': 145.6830, 'state': 'QLD', 'type': 'regional'},\n",
    "    'charters towers': {'lat': -20.0760, 'lon': 146.2610, 'state': 'QLD', 'type': 'regional'},\n",
    "    'ayr': {'lat': -19.5740, 'lon': 147.4040, 'state': 'QLD', 'type': 'regional'},\n",
    "    'ingham': {'lat': -18.6500, 'lon': 146.1670, 'state': 'QLD', 'type': 'regional'},\n",
    "    'innisfail': {'lat': -17.5240, 'lon': 146.0330, 'state': 'QLD', 'type': 'regional'},\n",
    "    'atherton': {'lat': -17.2670, 'lon': 145.4830, 'state': 'QLD', 'type': 'regional'},\n",
    "    'mareeba': {'lat': -17.0000, 'lon': 145.4330, 'state': 'QLD', 'type': 'regional'},\n",
    "    'biloela': {'lat': -24.4000, 'lon': 150.5170, 'state': 'QLD', 'type': 'regional'},\n",
    "    'monto': {'lat': -24.8670, 'lon': 151.1170, 'state': 'QLD', 'type': 'regional'},\n",
    "    'gayndah': {'lat': -25.6300, 'lon': 151.6170, 'state': 'QLD', 'type': 'regional'},\n",
    "    'murgon': {'lat': -26.2400, 'lon': 151.9400, 'state': 'QLD', 'type': 'regional'},\n",
    "    \n",
    "    # Regional VIC\n",
    "    'geelong': {'lat': -38.1499, 'lon': 144.3617, 'state': 'VIC', 'type': 'regional'},\n",
    "    'ballarat': {'lat': -37.5622, 'lon': 143.8503, 'state': 'VIC', 'type': 'regional'},\n",
    "    'bendigo': {'lat': -36.7570, 'lon': 144.2794, 'state': 'VIC', 'type': 'regional'},\n",
    "    'shepparton': {'lat': -36.3833, 'lon': 145.4000, 'state': 'VIC', 'type': 'regional'},\n",
    "    'mildura': {'lat': -34.2087, 'lon': 142.1311, 'state': 'VIC', 'type': 'regional'},\n",
    "    'warrnambool': {'lat': -38.3818, 'lon': 142.4876, 'state': 'VIC', 'type': 'regional'},\n",
    "    'wodonga': {'lat': -36.1217, 'lon': 146.8883, 'state': 'VIC', 'type': 'regional'},\n",
    "    'horsham': {'lat': -36.7117, 'lon': 142.2000, 'state': 'VIC', 'type': 'regional'},\n",
    "    'wangaratta': {'lat': -36.3578, 'lon': 146.3122, 'state': 'VIC', 'type': 'regional'},\n",
    "    'sale': {'lat': -38.1000, 'lon': 147.0667, 'state': 'VIC', 'type': 'regional'},\n",
    "    'traralgon': {'lat': -38.1950, 'lon': 146.5400, 'state': 'VIC', 'type': 'regional'},\n",
    "    'bairnsdale': {'lat': -37.8230, 'lon': 147.6100, 'state': 'VIC', 'type': 'regional'},\n",
    "    'echuca': {'lat': -36.1300, 'lon': 144.7500, 'state': 'VIC', 'type': 'regional'},\n",
    "    'swan hill': {'lat': -35.3380, 'lon': 143.5500, 'state': 'VIC', 'type': 'regional'},\n",
    "    'hamilton': {'lat': -37.7440, 'lon': 142.0220, 'state': 'VIC', 'type': 'regional'},\n",
    "    'colac': {'lat': -38.3400, 'lon': 143.5900, 'state': 'VIC', 'type': 'regional'},\n",
    "    'ararat': {'lat': -37.2840, 'lon': 142.9300, 'state': 'VIC', 'type': 'regional'},\n",
    "    'stawell': {'lat': -37.0560, 'lon': 142.7800, 'state': 'VIC', 'type': 'regional'},\n",
    "    'castlemaine': {'lat': -37.0670, 'lon': 144.2170, 'state': 'VIC', 'type': 'regional'},\n",
    "    'kyneton': {'lat': -37.2500, 'lon': 144.4500, 'state': 'VIC', 'type': 'regional'},\n",
    "    'seymour': {'lat': -37.0260, 'lon': 145.1390, 'state': 'VIC', 'type': 'regional'},\n",
    "    'benalla': {'lat': -36.5520, 'lon': 145.9830, 'state': 'VIC', 'type': 'regional'},\n",
    "    'cobram': {'lat': -35.9200, 'lon': 145.6500, 'state': 'VIC', 'type': 'regional'},\n",
    "    'yarrawonga': {'lat': -36.0300, 'lon': 146.0000, 'state': 'VIC', 'type': 'regional'},\n",
    "    'kyabram': {'lat': -36.3170, 'lon': 145.0500, 'state': 'VIC', 'type': 'regional'},\n",
    "    'rochester': {'lat': -36.3670, 'lon': 144.7000, 'state': 'VIC', 'type': 'regional'},\n",
    "    'kerang': {'lat': -35.7330, 'lon': 143.9170, 'state': 'VIC', 'type': 'regional'},\n",
    "    \n",
    "    # Regional SA\n",
    "    'mount gambier': {'lat': -37.8297, 'lon': 140.7811, 'state': 'SA', 'type': 'regional'},\n",
    "    'whyalla': {'lat': -33.0333, 'lon': 137.5167, 'state': 'SA', 'type': 'regional'},\n",
    "    'port lincoln': {'lat': -34.7333, 'lon': 135.8500, 'state': 'SA', 'type': 'regional'},\n",
    "    'port augusta': {'lat': -32.4936, 'lon': 137.7825, 'state': 'SA', 'type': 'regional'},\n",
    "    'port pirie': {'lat': -33.1858, 'lon': 138.0169, 'state': 'SA', 'type': 'regional'},\n",
    "    'murray bridge': {'lat': -35.1197, 'lon': 139.2756, 'state': 'SA', 'type': 'regional'},\n",
    "    'renmark': {'lat': -34.1760, 'lon': 140.7470, 'state': 'SA', 'type': 'regional'},\n",
    "    'berri': {'lat': -34.2830, 'lon': 140.6000, 'state': 'SA', 'type': 'regional'},\n",
    "    'loxton': {'lat': -34.4500, 'lon': 140.5670, 'state': 'SA', 'type': 'regional'},\n",
    "    'kadina': {'lat': -33.9670, 'lon': 137.7170, 'state': 'SA', 'type': 'regional'},\n",
    "    'clare': {'lat': -33.8330, 'lon': 138.6000, 'state': 'SA', 'type': 'regional'},\n",
    "    'tanunda': {'lat': -34.5230, 'lon': 138.9600, 'state': 'SA', 'type': 'regional'},\n",
    "    'nuriootpa': {'lat': -34.4670, 'lon': 139.0000, 'state': 'SA', 'type': 'regional'},\n",
    "    'naracoorte': {'lat': -36.9500, 'lon': 140.7330, 'state': 'SA', 'type': 'regional'},\n",
    "    'bordertown': {'lat': -36.3100, 'lon': 140.7700, 'state': 'SA', 'type': 'regional'},\n",
    "    \n",
    "    # Regional WA\n",
    "    'bunbury': {'lat': -33.3261, 'lon': 115.6394, 'state': 'WA', 'type': 'regional'},\n",
    "    'geraldton': {'lat': -28.7775, 'lon': 114.6147, 'state': 'WA', 'type': 'regional'},\n",
    "    'kalgoorlie': {'lat': -30.7489, 'lon': 121.4658, 'state': 'WA', 'type': 'regional'},\n",
    "    'albany': {'lat': -35.0275, 'lon': 117.8839, 'state': 'WA', 'type': 'regional'},\n",
    "    'mandurah': {'lat': -32.5269, 'lon': 115.7217, 'state': 'WA', 'type': 'regional'},\n",
    "    'broome': {'lat': -17.9614, 'lon': 122.2359, 'state': 'WA', 'type': 'regional'},\n",
    "    'karratha': {'lat': -20.7361, 'lon': 116.8467, 'state': 'WA', 'type': 'regional'},\n",
    "    'port hedland': {'lat': -20.3108, 'lon': 118.5753, 'state': 'WA', 'type': 'regional'},\n",
    "    'esperance': {'lat': -33.8611, 'lon': 121.8919, 'state': 'WA', 'type': 'regional'},\n",
    "    'carnarvon': {'lat': -24.8840, 'lon': 113.6590, 'state': 'WA', 'type': 'regional'},\n",
    "    'kununurra': {'lat': -15.7730, 'lon': 128.7380, 'state': 'WA', 'type': 'regional'},\n",
    "    'collie': {'lat': -33.3600, 'lon': 116.1500, 'state': 'WA', 'type': 'regional'},\n",
    "    'katanning': {'lat': -33.6900, 'lon': 117.5500, 'state': 'WA', 'type': 'regional'},\n",
    "    'narrogin': {'lat': -32.9330, 'lon': 117.1830, 'state': 'WA', 'type': 'regional'},\n",
    "    'merredin': {'lat': -31.4830, 'lon': 118.2830, 'state': 'WA', 'type': 'regional'},\n",
    "    'northam': {'lat': -31.6500, 'lon': 116.6670, 'state': 'WA', 'type': 'regional'},\n",
    "    'moora': {'lat': -30.6400, 'lon': 116.0170, 'state': 'WA', 'type': 'regional'},\n",
    "    'dalwallinu': {'lat': -30.2770, 'lon': 116.6630, 'state': 'WA', 'type': 'regional'},\n",
    "    \n",
    "    # Regional NT\n",
    "    'alice springs': {'lat': -23.6980, 'lon': 133.8807, 'state': 'NT', 'type': 'regional'},\n",
    "    'katherine': {'lat': -14.4650, 'lon': 132.2635, 'state': 'NT', 'type': 'regional'},\n",
    "    'tennant creek': {'lat': -19.6497, 'lon': 134.1917, 'state': 'NT', 'type': 'regional'},\n",
    "    \n",
    "    # Regional TAS\n",
    "    'launceston': {'lat': -41.4332, 'lon': 147.1441, 'state': 'TAS', 'type': 'regional'},\n",
    "    'devonport': {'lat': -41.1760, 'lon': 146.3510, 'state': 'TAS', 'type': 'regional'},\n",
    "    'burnie': {'lat': -41.0556, 'lon': 145.9056, 'state': 'TAS', 'type': 'regional'},\n",
    "    \n",
    "    # Airports (IATA codes)\n",
    "    'syd': {'lat': -33.9399, 'lon': 151.1753, 'state': 'NSW', 'type': 'airport'},\n",
    "    'mel': {'lat': -37.6690, 'lon': 144.8410, 'state': 'VIC', 'type': 'airport'},\n",
    "    'bne': {'lat': -27.3942, 'lon': 153.1218, 'state': 'QLD', 'type': 'airport'},\n",
    "    'per': {'lat': -31.9403, 'lon': 115.9670, 'state': 'WA', 'type': 'airport'},\n",
    "    'adl': {'lat': -34.9450, 'lon': 138.5306, 'state': 'SA', 'type': 'airport'},\n",
    "    'cbr': {'lat': -35.3069, 'lon': 149.1950, 'state': 'ACT', 'type': 'airport'},\n",
    "    'ool': {'lat': -28.1644, 'lon': 153.5047, 'state': 'QLD', 'type': 'airport'},  # Gold Coast\n",
    "    'cns': {'lat': -16.8858, 'lon': 145.7553, 'state': 'QLD', 'type': 'airport'},  # Cairns\n",
    "    'tsv': {'lat': -19.2525, 'lon': 146.7656, 'state': 'QLD', 'type': 'airport'},  # Townsville\n",
    "    'dbo': {'lat': -32.2167, 'lon': 148.5747, 'state': 'NSW', 'type': 'airport'},  # Dubbo\n",
    "    'ntl': {'lat': -32.7950, 'lon': 151.8342, 'state': 'NSW', 'type': 'airport'},  # Newcastle\n",
    "    'rce': {'lat': -23.3819, 'lon': 150.4753, 'state': 'QLD', 'type': 'airport'},  # Rockhampton\n",
    "    'mky': {'lat': -21.1717, 'lon': 149.1797, 'state': 'QLD', 'type': 'airport'},  # Mackay\n",
    "    'isa': {'lat': -20.6639, 'lon': 139.4886, 'state': 'QLD', 'type': 'airport'},  # Mount Isa\n",
    "    'eme': {'lat': -23.5675, 'lon': 148.1792, 'state': 'QLD', 'type': 'airport'},  # Emerald\n",
    "    'lre': {'lat': -23.4342, 'lon': 144.2797, 'state': 'QLD', 'type': 'airport'},  # Longreach\n",
    "    \n",
    "    # International destinations (common business travel)\n",
    "    'utrecht': {'lat': 52.0907, 'lon': 5.1214, 'state': 'NL', 'type': 'international'},\n",
    "    'amsterdam': {'lat': 52.3676, 'lon': 4.9041, 'state': 'NL', 'type': 'international'},\n",
    "    'netherlands': {'lat': 52.1326, 'lon': 5.2913, 'state': 'NL', 'type': 'international'},\n",
    "    'singapore': {'lat': 1.3521, 'lon': 103.8198, 'state': 'SG', 'type': 'international'},\n",
    "    'hong kong': {'lat': 22.3193, 'lon': 114.1694, 'state': 'HK', 'type': 'international'},\n",
    "    'new zealand': {'lat': -40.9006, 'lon': 174.8860, 'state': 'NZ', 'type': 'international'},\n",
    "    'auckland': {'lat': -36.8509, 'lon': 174.7645, 'state': 'NZ', 'type': 'international'},\n",
    "    'wellington': {'lat': -41.2924, 'lon': 174.7787, 'state': 'NZ', 'type': 'international'},\n",
    "    'london': {'lat': 51.5074, 'lon': -0.1278, 'state': 'UK', 'type': 'international'},\n",
    "    'boston': {'lat': 42.3601, 'lon': -71.0589, 'state': 'US', 'type': 'international'},\n",
    "    'new york': {'lat': 40.7128, 'lon': -74.0060, 'state': 'US', 'type': 'international'},\n",
    "    'tokyo': {'lat': 35.6762, 'lon': 139.6503, 'state': 'JP', 'type': 'international'},\n",
    "    'beijing': {'lat': 39.9042, 'lon': 116.4074, 'state': 'CN', 'type': 'international'},\n",
    "    'shanghai': {'lat': 31.2304, 'lon': 121.4737, 'state': 'CN', 'type': 'international'},\n",
    "    'jakarta': {'lat': -6.2088, 'lon': 106.8456, 'state': 'ID', 'type': 'international'},\n",
    "    'manila': {'lat': 14.5995, 'lon': 120.9842, 'state': 'PH', 'type': 'international'},\n",
    "    'bangkok': {'lat': 13.7563, 'lon': 100.5018, 'state': 'TH', 'type': 'international'},\n",
    "    'mumbai': {'lat': 19.0760, 'lon': 72.8777, 'state': 'IN', 'type': 'international'},\n",
    "    'dubai': {'lat': 25.2048, 'lon': 55.2708, 'state': 'AE', 'type': 'international'},\n",
    "}\n",
    "\n",
    "# State abbreviations\n",
    "STATE_MAPPING = {\n",
    "    'nsw': 'NSW', 'new south wales': 'NSW',\n",
    "    'vic': 'VIC', 'victoria': 'VIC',\n",
    "    'qld': 'QLD', 'queensland': 'QLD',\n",
    "    'wa': 'WA', 'western australia': 'WA',\n",
    "    'sa': 'SA', 'south australia': 'SA',\n",
    "    'tas': 'TAS', 'tasmania': 'TAS',\n",
    "    'nt': 'NT', 'northern territory': 'NT',\n",
    "    'act': 'ACT', 'australian capital territory': 'ACT'\n",
    "}\n",
    "\n",
    "print(f\"Loaded {len(AUSTRALIAN_LOCATIONS)} locations in database\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 174 locations in database\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Location Extraction and Geocoding"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-05T08:21:19.458064Z",
     "start_time": "2025-12-05T08:21:19.440533Z"
    }
   },
   "source": [
    "class LocationExtractor:\n",
    "    \"\"\"\n",
    "    Extract and geocode locations from expense descriptions.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, reference_location: Dict = None):\n",
    "        self.locations_db = AUSTRALIAN_LOCATIONS\n",
    "        self.reference = reference_location or CONFIG['reference_location']\n",
    "        self.cache = {}\n",
    "        \n",
    "        # Build pattern for location matching\n",
    "        location_names = sorted(self.locations_db.keys(), key=len, reverse=True)\n",
    "        escaped_names = [re.escape(name) for name in location_names]\n",
    "        self.location_pattern = re.compile(\n",
    "            r'\\b(' + '|'.join(escaped_names) + r')\\b',\n",
    "            re.IGNORECASE\n",
    "        )\n",
    "        \n",
    "        # Initialize geocoder for unknown locations (if available)\n",
    "        if GEOPY_AVAILABLE:\n",
    "            self.geocoder = Nominatim(user_agent=\"fbt_classifier\")\n",
    "        else:\n",
    "            self.geocoder = None\n",
    "    \n",
    "    def extract_locations(self, text: str) -> List[str]:\n",
    "        \"\"\"\n",
    "        Extract location names from text.\n",
    "        \"\"\"\n",
    "        if pd.isna(text) or not text:\n",
    "            return []\n",
    "        \n",
    "        text = str(text).lower()\n",
    "        matches = self.location_pattern.findall(text)\n",
    "        return list(set(matches))\n",
    "    \n",
    "    def get_coordinates(self, location_name: str) -> Optional[Dict]:\n",
    "        \"\"\"\n",
    "        Get coordinates for a location name.\n",
    "        \"\"\"\n",
    "        location_name = location_name.lower().strip()\n",
    "        \n",
    "        # Check cache first\n",
    "        if location_name in self.cache:\n",
    "            return self.cache[location_name]\n",
    "        \n",
    "        # Check database\n",
    "        if location_name in self.locations_db:\n",
    "            coords = self.locations_db[location_name]\n",
    "            self.cache[location_name] = coords\n",
    "            return coords\n",
    "        \n",
    "        # Try geocoding unknown locations (with rate limiting)\n",
    "        if self.geocoder and CONFIG.get('enable_geocoding', False):\n",
    "            try:\n",
    "                result = self.geocoder.geocode(f\"{location_name}, Australia\", timeout=5)\n",
    "                if result:\n",
    "                    coords = {'lat': result.latitude, 'lon': result.longitude, \n",
    "                              'state': 'UNKNOWN', 'type': 'geocoded'}\n",
    "                    self.cache[location_name] = coords\n",
    "                    return coords\n",
    "            except Exception:\n",
    "                pass\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    def calculate_distance(self, lat1: float, lon1: float, \n",
    "                           lat2: float, lon2: float) -> float:\n",
    "        \"\"\"\n",
    "        Calculate distance between two points using Haversine formula.\n",
    "        Returns distance in kilometers.\n",
    "        \"\"\"\n",
    "        R = 6371  # Earth's radius in km\n",
    "        \n",
    "        lat1_rad = math.radians(lat1)\n",
    "        lat2_rad = math.radians(lat2)\n",
    "        delta_lat = math.radians(lat2 - lat1)\n",
    "        delta_lon = math.radians(lon2 - lon1)\n",
    "        \n",
    "        a = (math.sin(delta_lat/2)**2 + \n",
    "             math.cos(lat1_rad) * math.cos(lat2_rad) * math.sin(delta_lon/2)**2)\n",
    "        c = 2 * math.atan2(math.sqrt(a), math.sqrt(1-a))\n",
    "        \n",
    "        return R * c\n",
    "    \n",
    "    def estimate_travel_time(self, distance_km: float, travel_type: str = 'auto') -> float:\n",
    "        \"\"\"\n",
    "        Estimate travel time in hours based on distance.\n",
    "        \"\"\"\n",
    "        if distance_km <= 0:\n",
    "            return 0\n",
    "        \n",
    "        # Rough estimates\n",
    "        if travel_type == 'flight' or distance_km > 500:\n",
    "            # Flight time estimate (800 km/h average + 2h airport time)\n",
    "            return (distance_km / 800) + 2\n",
    "        elif travel_type == 'car' or distance_km <= 500:\n",
    "            # Driving estimate (80 km/h average including stops)\n",
    "            return distance_km / 80\n",
    "        else:\n",
    "            # Default to car\n",
    "            return distance_km / 80\n",
    "    \n",
    "    def extract_location_features(self, text: str) -> Dict:\n",
    "        \"\"\"\n",
    "        Extract all location-related features from text.\n",
    "        \"\"\"\n",
    "        features = {\n",
    "            'locations_found': 0,\n",
    "            'location_names': '',\n",
    "            'primary_location': '',\n",
    "            'primary_state': '',\n",
    "            'primary_lat': np.nan,\n",
    "            'primary_lon': np.nan,\n",
    "            'distance_from_ref_km': np.nan,\n",
    "            'estimated_travel_hours': np.nan,\n",
    "            'is_international': 0,\n",
    "            'is_regional': 0,\n",
    "            'is_major_city': 0,\n",
    "            'is_local': 0,  # Within 100km\n",
    "            'is_interstate': 0,\n",
    "            'is_remote': 0,  # > 500km\n",
    "            'travel_category': 'unknown'\n",
    "        }\n",
    "        \n",
    "        locations = self.extract_locations(text)\n",
    "        features['locations_found'] = len(locations)\n",
    "        \n",
    "        if not locations:\n",
    "            return features\n",
    "        \n",
    "        features['location_names'] = ', '.join(locations)\n",
    "        \n",
    "        # Use first found location as primary\n",
    "        primary = locations[0]\n",
    "        features['primary_location'] = primary\n",
    "        \n",
    "        coords = self.get_coordinates(primary)\n",
    "        if coords:\n",
    "            features['primary_lat'] = coords['lat']\n",
    "            features['primary_lon'] = coords['lon']\n",
    "            features['primary_state'] = coords.get('state', '')\n",
    "            \n",
    "            # Location type flags\n",
    "            loc_type = coords.get('type', '')\n",
    "            features['is_international'] = int(loc_type == 'international')\n",
    "            features['is_regional'] = int(loc_type == 'regional')\n",
    "            features['is_major_city'] = int(loc_type == 'city')\n",
    "            \n",
    "            # Calculate distance from reference\n",
    "            distance = self.calculate_distance(\n",
    "                self.reference['lat'], self.reference['lon'],\n",
    "                coords['lat'], coords['lon']\n",
    "            )\n",
    "            features['distance_from_ref_km'] = round(distance, 2)\n",
    "            \n",
    "            # Travel time estimate\n",
    "            travel_type = 'flight' if distance > 500 or features['is_international'] else 'car'\n",
    "            features['estimated_travel_hours'] = round(\n",
    "                self.estimate_travel_time(distance, travel_type), 2\n",
    "            )\n",
    "            \n",
    "            # Distance categories\n",
    "            features['is_local'] = int(distance <= 100)\n",
    "            features['is_remote'] = int(distance > 500)\n",
    "            features['is_interstate'] = int(\n",
    "                coords.get('state', 'NSW') != 'NSW' and not features['is_international']\n",
    "            )\n",
    "            \n",
    "            # Travel category\n",
    "            if features['is_international']:\n",
    "                features['travel_category'] = 'international'\n",
    "            elif features['is_remote']:\n",
    "                features['travel_category'] = 'remote'\n",
    "            elif features['is_interstate']:\n",
    "                features['travel_category'] = 'interstate'\n",
    "            elif features['is_regional']:\n",
    "                features['travel_category'] = 'regional'\n",
    "            elif features['is_local']:\n",
    "                features['travel_category'] = 'local'\n",
    "            else:\n",
    "                features['travel_category'] = 'domestic'\n",
    "        \n",
    "        return features"
   ],
   "outputs": [],
   "execution_count": 17
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-05T08:21:20.548440Z",
     "start_time": "2025-12-05T08:21:20.517642Z"
    }
   },
   "source": [
    "# Test location extraction\n",
    "loc_extractor = LocationExtractor()\n",
    "\n",
    "test_texts = [\n",
    "    \"Flight to Brisbane for client meeting\",\n",
    "    \"Dinner at Roma with clients - Board Members Visit\",\n",
    "    \"Trip to Utrecht for Global General Counsel Offsite\",\n",
    "    \"Taxi to airport for Sydney trip\",\n",
    "    \"Team lunch at local restaurant\",\n",
    "    \"Travel to Dubbo for farm visit\",\n",
    "    \"Client dinner in Melbourne CBD\"\n",
    "]\n",
    "\n",
    "print(\"Location Extraction Test:\")\n",
    "print(\"=\" * 70)\n",
    "for text in test_texts:\n",
    "    features = loc_extractor.extract_location_features(text)\n",
    "    print(f\"\\nText: {text}\")\n",
    "    print(f\"  Location: {features['primary_location']}\")\n",
    "    print(f\"  Distance: {features['distance_from_ref_km']} km\")\n",
    "    print(f\"  Travel Time: {features['estimated_travel_hours']} hrs\")\n",
    "    print(f\"  Category: {features['travel_category']}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Location Extraction Test:\n",
      "======================================================================\n",
      "\n",
      "Text: Flight to Brisbane for client meeting\n",
      "  Location: brisbane\n",
      "  Distance: 732.38 km\n",
      "  Travel Time: 2.92 hrs\n",
      "  Category: remote\n",
      "\n",
      "Text: Dinner at Roma with clients - Board Members Visit\n",
      "  Location: roma\n",
      "  Distance: 844.27 km\n",
      "  Travel Time: 3.06 hrs\n",
      "  Category: remote\n",
      "\n",
      "Text: Trip to Utrecht for Global General Counsel Offsite\n",
      "  Location: utrecht\n",
      "  Distance: 16641.8 km\n",
      "  Travel Time: 22.8 hrs\n",
      "  Category: international\n",
      "\n",
      "Text: Taxi to airport for Sydney trip\n",
      "  Location: sydney\n",
      "  Distance: 0.0 km\n",
      "  Travel Time: 0 hrs\n",
      "  Category: local\n",
      "\n",
      "Text: Team lunch at local restaurant\n",
      "  Location: \n",
      "  Distance: nan km\n",
      "  Travel Time: nan hrs\n",
      "  Category: unknown\n",
      "\n",
      "Text: Travel to Dubbo for farm visit\n",
      "  Location: dubbo\n",
      "  Distance: 301.98 km\n",
      "  Travel Time: 3.77 hrs\n",
      "  Category: regional\n",
      "\n",
      "Text: Client dinner in Melbourne CBD\n",
      "  Location: melbourne\n",
      "  Distance: 713.43 km\n",
      "  Travel Time: 2.89 hrs\n",
      "  Category: remote\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-05T08:21:21.779767Z",
     "start_time": "2025-12-05T08:21:21.755183Z"
    }
   },
   "source": [
    "def load_raw_data(filepath: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load raw transaction data from all sheets.\n",
    "    \"\"\"\n",
    "    print(f\"Loading raw data from: {filepath}\")\n",
    "    xl = pd.ExcelFile(filepath)\n",
    "    print(f\"Found {len(xl.sheet_names)} sheets\")\n",
    "    \n",
    "    dfs = []\n",
    "    for sheet in xl.sheet_names:\n",
    "        df = pd.read_excel(filepath, sheet_name=sheet)\n",
    "        df['_source_sheet'] = sheet\n",
    "        dfs.append(df)\n",
    "        print(f\"  - {sheet}: {len(df)} rows\")\n",
    "    \n",
    "    combined = pd.concat(dfs, ignore_index=True)\n",
    "    print(f\"\\nTotal: {len(combined)} rows\")\n",
    "    return combined\n",
    "\n",
    "\n",
    "def parse_workpaper(filepath: str, sheet_name: str) -> Optional[pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Parse a workpaper sheet to extract labeled transactions.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df = pd.read_excel(filepath, sheet_name=sheet_name, header=None)\n",
    "        \n",
    "        # Find header row\n",
    "        header_idx = None\n",
    "        for i, row in df.iterrows():\n",
    "            row_str = ' '.join([str(v).upper() for v in row.values if pd.notna(v)])\n",
    "            if 'BUSINESS_UNIT_CODE' in row_str:\n",
    "                header_idx = i\n",
    "                break\n",
    "        \n",
    "        if header_idx is None:\n",
    "            return None\n",
    "        \n",
    "        # Set headers\n",
    "        headers = df.iloc[header_idx].tolist()\n",
    "        headers = [str(h).strip() if pd.notna(h) else f'_col_{i}' for i, h in enumerate(headers)]\n",
    "        \n",
    "        data = df.iloc[header_idx + 1:].copy()\n",
    "        data.columns = headers\n",
    "        \n",
    "        # Find label columns\n",
    "        col_mapping = {}\n",
    "        for col in headers:\n",
    "            col_lower = str(col).lower()\n",
    "            if 'tax' in col_lower and 'desc' in col_lower:\n",
    "                col_mapping[col] = 'tax_description'\n",
    "            elif 'me?' in col_lower:\n",
    "                col_mapping[col] = 'me_label'\n",
    "        \n",
    "        data = data.rename(columns=col_mapping)\n",
    "        data['_source_file'] = filepath.split('/')[-1]\n",
    "        data['_source_sheet'] = sheet_name\n",
    "        \n",
    "        return data\n",
    "    except Exception as e:\n",
    "        print(f\"  Error: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def load_labeled_data(wp_files: List[str]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load labeled data from all workpapers.\n",
    "    \"\"\"\n",
    "    all_dfs = []\n",
    "    \n",
    "    for filepath in wp_files:\n",
    "        print(f\"\\nProcessing: {filepath}\")\n",
    "        try:\n",
    "            xl = pd.ExcelFile(filepath)\n",
    "            for sheet in xl.sheet_names:\n",
    "                if any(skip in sheet.lower() for skip in ['summary', 'trial balance']):\n",
    "                    continue\n",
    "                \n",
    "                df = parse_workpaper(filepath, sheet)\n",
    "                if df is not None and len(df) > 0:\n",
    "                    has_me = 'me_label' in df.columns\n",
    "                    print(f\"  - {sheet}: {len(df)} rows (has labels: {has_me})\")\n",
    "                    all_dfs.append(df)\n",
    "        except Exception as e:\n",
    "            print(f\"  Error: {e}\")\n",
    "    \n",
    "    if not all_dfs:\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    combined = pd.concat(all_dfs, ignore_index=True)\n",
    "    print(f\"\\nTotal labeled: {len(combined)} rows\")\n",
    "    return combined"
   ],
   "outputs": [],
   "execution_count": 19
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-05T08:22:10.486982Z",
     "start_time": "2025-12-05T08:21:23.291446Z"
    }
   },
   "source": [
    "# Load data\n",
    "raw_data = load_raw_data(CONFIG['raw_data_path'])\n",
    "labeled_data = load_labeled_data(CONFIG['wp_files'])"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading raw data from: data/data_raw_2024-25.xlsx\n",
      "Found 14 sheets\n",
      "  - 5130006000: 224 rows\n",
      "  - 5130002000: 4347 rows\n",
      "  - 5130005000: 3673 rows\n",
      "  - 5130004000: 27108 rows\n",
      "  - 5130001000: 3097 rows\n",
      "  - 5130003000: 1165 rows\n",
      "  - 5140001010: 349 rows\n",
      "  - 5140001030: 114 rows\n",
      "  - 5160001000: 570 rows\n",
      "  - 5160007000: 76 rows\n",
      "  - 5160008000: 48 rows\n",
      "  - 5160010000: 247 rows\n",
      "  - 5160011000: 1311 rows\n",
      "  - Misc: 5062 rows\n",
      "\n",
      "Total: 47391 rows\n",
      "\n",
      "Processing: data/WP_1_Apr_to_Dec_24_FBT_ent_acc.xlsx\n",
      "  - Travel&lodging: 33218 rows (has labels: True)\n",
      "  - Meals Exp: 22080 rows (has labels: True)\n",
      "  - Lodging Exp: 3083 rows (has labels: True)\n",
      "  - Dom Pub Trnsprt: 3699 rows (has labels: True)\n",
      "  - Travel Exp Abroad: 1013 rows (has labels: True)\n",
      "  - Ent Exp: 163 rows (has labels: True)\n",
      "\n",
      "Processing: data/WP_2_Apr_to_Dec_24_FBT_ent_acc.xlsx\n",
      "  - Ext Training: 296 rows (has labels: True)\n",
      "  - Training Travel&lodging: 101 rows (has labels: True)\n",
      "  - Publicity&Advertising: 401 rows (has labels: True)\n",
      "  - Sponsoring: 70 rows (has labels: True)\n",
      "  - Promotional gifts: 38 rows (has labels: True)\n",
      "  - Marketing exp: 183 rows (has labels: True)\n",
      "  - Business Dev: 1104 rows (has labels: True)\n",
      "  - Public relations: 37 rows (has labels: True)\n",
      "  - Membership: 224 rows (has labels: True)\n",
      "  - Meeting costs: 154 rows (has labels: True)\n",
      "  - Misc exp: 377 rows (has labels: True)\n",
      "  - Sundry exp: 28 rows (has labels: True)\n",
      "  - Misc exp (2): 1229 rows (has labels: True)\n",
      "  - Other non-bank exp: 224 rows (has labels: True)\n",
      "  - Other exp: 1817 rows (has labels: True)\n",
      "  - Donations: 70 rows (has labels: True)\n",
      "  - Client period: 22 rows (has labels: True)\n",
      "\n",
      "Processing: data/WP_3_Jan_to_Mar_25_FBT_ent_acc.xlsx\n",
      "  - Travel&lodging: 31357 rows (has labels: True)\n",
      "  - Meals Exp: 5029 rows (has labels: True)\n",
      "  - Lodging Exp: 590 rows (has labels: True)\n",
      "  - Dom Pub Trnsprt: 648 rows (has labels: True)\n",
      "  - Travel Exp Abroad: 152 rows (has labels: True)\n",
      "  - Ent Exp: 61 rows (has labels: True)\n",
      "\n",
      "Processing: data/WP_4_Jan_to_Mar_25_FBT_ent_acc.xlsx\n",
      "  - Ext Training: 53 rows (has labels: True)\n",
      "  - Training Trav&lodg: 13 rows (has labels: True)\n",
      "  - Publicity&Advert'g: 169 rows (has labels: True)\n",
      "  - Promo gifts: 10 rows (has labels: True)\n",
      "  - Sponsoring: 6 rows (has labels: True)\n",
      "  - Marketing: 64 rows (has labels: True)\n",
      "  - Bus Dev: 207 rows (has labels: True)\n",
      "  - PR exp: 9 rows (has labels: True)\n",
      "  - Club memship: 38 rows (has labels: True)\n",
      "  - Meeting costs: 53 rows (has labels: True)\n",
      "  - Misc exp: 214 rows (has labels: True)\n",
      "  - Misc exp (2): 101 rows (has labels: True)\n",
      "  - Other non-bank exp: 23 rows (has labels: True)\n",
      "  - Other exp: 587 rows (has labels: True)\n",
      "  - Donations: 14 rows (has labels: True)\n",
      "  - Client period: 5 rows (has labels: True)\n",
      "\n",
      "Total labeled: 109034 rows\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-05T08:22:12.012171Z",
     "start_time": "2025-12-05T08:22:11.983326Z"
    }
   },
   "source": [
    "class DataCleaner:\n",
    "    \"\"\"\n",
    "    Comprehensive data cleaning.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.stats = {}\n",
    "    \n",
    "    def standardize_columns(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        df = df.copy()\n",
    "        df = df.loc[:, ~df.columns.str.contains('^Unnamed|^_col_', case=False, na=False)]\n",
    "        \n",
    "        new_cols = []\n",
    "        for col in df.columns:\n",
    "            new_col = str(col).strip()\n",
    "            new_col = re.sub(r'\\s+', '_', new_col)\n",
    "            new_col = re.sub(r'[^a-zA-Z0-9_]', '', new_col)\n",
    "            new_col = new_col.upper()\n",
    "            new_cols.append(new_col)\n",
    "        \n",
    "        df.columns = new_cols\n",
    "        return df\n",
    "    \n",
    "    def clean_text(self, series: pd.Series) -> pd.Series:\n",
    "        series = series.fillna('').astype(str).str.strip()\n",
    "        series = series.str.replace(r'\\s+', ' ', regex=True)\n",
    "        series = series.replace(['nan', 'NaN', 'None', ''], np.nan)\n",
    "        return series\n",
    "    \n",
    "    def clean_me_label(self, series: pd.Series) -> pd.Series:\n",
    "        series = series.fillna('').astype(str).str.strip().str.upper()\n",
    "        mapping = {\n",
    "            'Y': 'Y', 'YES': 'Y', '1': 'Y', 'TRUE': 'Y',\n",
    "            'N': 'N', 'NO': 'N', '0': 'N', 'FALSE': 'N'\n",
    "        }\n",
    "        return series.map(lambda x: mapping.get(x, np.nan))\n",
    "    \n",
    "    def clean(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        print(f\"Cleaning {len(df)} rows...\")\n",
    "        initial = len(df)\n",
    "        \n",
    "        df = self.standardize_columns(df)\n",
    "        \n",
    "        # Clean text columns\n",
    "        text_cols = ['PURPOSE', 'CHARGE_DESCRIPTION', 'LINE_DESCR', \n",
    "                     'DESCRIPTION', 'INVOICE_DESCR', 'VENDOR_NAME', 'DESCR']\n",
    "        for col in text_cols:\n",
    "            if col in df.columns:\n",
    "                df[col] = self.clean_text(df[col])\n",
    "        \n",
    "        # Clean numeric\n",
    "        if 'BASE_AMOUNT' in df.columns:\n",
    "            df['BASE_AMOUNT'] = pd.to_numeric(df['BASE_AMOUNT'], errors='coerce')\n",
    "        \n",
    "        # Clean dates\n",
    "        if 'JOURNAL_DATE' in df.columns:\n",
    "            df['JOURNAL_DATE'] = pd.to_datetime(df['JOURNAL_DATE'], errors='coerce')\n",
    "        \n",
    "        # Clean labels\n",
    "        if 'ME_LABEL' in df.columns:\n",
    "            df['ME_LABEL'] = self.clean_me_label(df['ME_LABEL'])\n",
    "        if 'me_label' in df.columns:\n",
    "            df['me_label'] = self.clean_me_label(df['me_label'])\n",
    "        \n",
    "        # Remove duplicates\n",
    "        dedup_cols = ['BUSINESS_UNIT_CODE', 'ACCOUNT_CODE', 'BASE_AMOUNT', 'JOURNAL_DATE']\n",
    "        available = [c for c in dedup_cols if c in df.columns]\n",
    "        if available:\n",
    "            df = df.drop_duplicates(subset=available, keep='first')\n",
    "        \n",
    "        # Remove rows with all empty text\n",
    "        available_text = [c for c in text_cols if c in df.columns]\n",
    "        if available_text:\n",
    "            mask = df[available_text].notna().any(axis=1)\n",
    "            df = df[mask]\n",
    "        \n",
    "        print(f\"Cleaned: {len(df)} rows (removed {initial - len(df)})\")\n",
    "        return df"
   ],
   "outputs": [],
   "execution_count": 23
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-05T08:22:11.951090Z",
     "start_time": "2025-12-05T08:22:10.550565Z"
    }
   },
   "source": [
    "cleaner = DataCleaner()\n",
    "raw_data_clean = cleaner.clean(raw_data.copy())\n",
    "labeled_data_clean = cleaner.clean(labeled_data.copy())"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning 47391 rows...\n",
      "Cleaned: 33616 rows (removed 13775)\n",
      "Cleaning 109034 rows...\n",
      "Cleaned: 33678 rows (removed 75356)\n"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-05T08:20:33.196442Z",
     "start_time": "2025-12-05T08:20:32.640326Z"
    }
   },
   "source": [
    "# Check label distribution\n",
    "print(\"\\nLabel Distribution:\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "me_col = 'me_label' if 'me_label' in labeled_data_clean.columns else 'ME_LABEL'\n",
    "if me_col in labeled_data_clean.columns:\n",
    "    print(f\"\\n{me_col}:\")\n",
    "    print(labeled_data_clean[me_col].value_counts(dropna=False))\n",
    "\n",
    "tax_col = 'tax_description' if 'tax_description' in labeled_data_clean.columns else 'TAX_DESCRIPTION'\n",
    "if tax_col in labeled_data_clean.columns:\n",
    "    print(f\"\\nTop 15 {tax_col}:\")\n",
    "    print(labeled_data_clean[tax_col].value_counts().head(15))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Label Distribution:\n",
      "========================================\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'labeled_data_clean' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[9], line 5\u001B[0m\n\u001B[0;32m      2\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124mLabel Distribution:\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m      3\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m=\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m*\u001B[39m \u001B[38;5;241m40\u001B[39m)\n\u001B[1;32m----> 5\u001B[0m me_col \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mme_label\u001B[39m\u001B[38;5;124m'\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mme_label\u001B[39m\u001B[38;5;124m'\u001B[39m \u001B[38;5;129;01min\u001B[39;00m \u001B[43mlabeled_data_clean\u001B[49m\u001B[38;5;241m.\u001B[39mcolumns \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mME_LABEL\u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[0;32m      6\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m me_col \u001B[38;5;129;01min\u001B[39;00m labeled_data_clean\u001B[38;5;241m.\u001B[39mcolumns:\n\u001B[0;32m      7\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;132;01m{\u001B[39;00mme_col\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m:\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "\u001B[1;31mNameError\u001B[0m: name 'labeled_data_clean' is not defined"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-05T08:20:33.462144Z",
     "start_time": "2025-12-05T08:20:33.432597Z"
    }
   },
   "source": [
    "class FeatureEngineer:\n",
    "    \"\"\"\n",
    "    Comprehensive feature engineering.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.location_extractor = LocationExtractor()\n",
    "    \n",
    "    def create_combined_text(self, df: pd.DataFrame) -> pd.Series:\n",
    "        text_cols = ['PURPOSE', 'CHARGE_DESCRIPTION', 'LINE_DESCR',\n",
    "                     'DESCRIPTION', 'INVOICE_DESCR', 'VENDOR_NAME', 'DESCR']\n",
    "        available = [c for c in text_cols if c in df.columns]\n",
    "        \n",
    "        def combine(row):\n",
    "            parts = [str(row.get(c, '')).strip() for c in available \n",
    "                     if pd.notna(row.get(c)) and str(row.get(c)).strip()]\n",
    "            return ' '.join(parts)\n",
    "        \n",
    "        combined = df.apply(combine, axis=1)\n",
    "        \n",
    "        # Clean text\n",
    "        combined = combined.str.lower()\n",
    "        combined = combined.str.replace(r'[^a-z0-9\\s]', ' ', regex=True)\n",
    "        combined = combined.str.replace(r'\\s+', ' ', regex=True).str.strip()\n",
    "        \n",
    "        return combined\n",
    "    \n",
    "    def extract_amount_features(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        features = pd.DataFrame(index=df.index)\n",
    "        \n",
    "        if 'BASE_AMOUNT' in df.columns:\n",
    "            amt = df['BASE_AMOUNT'].fillna(0)\n",
    "            features['amount'] = amt\n",
    "            features['amount_abs'] = np.abs(amt)\n",
    "            features['amount_log'] = np.log1p(np.abs(amt))\n",
    "            features['amount_is_negative'] = (amt < 0).astype(int)\n",
    "            features['amount_is_round_100'] = (amt % 100 == 0).astype(int)\n",
    "            features['amount_is_round_50'] = (amt % 50 == 0).astype(int)\n",
    "            \n",
    "            # Buckets\n",
    "            features['amount_bucket'] = pd.cut(\n",
    "                np.abs(amt),\n",
    "                bins=[-np.inf, 50, 100, 200, 300, 500, 1000, 5000, np.inf],\n",
    "                labels=['0-50', '50-100', '100-200', '200-300', '300-500', \n",
    "                        '500-1000', '1000-5000', '5000+']\n",
    "            ).astype(str)\n",
    "        \n",
    "        return features\n",
    "    \n",
    "    def extract_date_features(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        features = pd.DataFrame(index=df.index)\n",
    "        \n",
    "        if 'JOURNAL_DATE' in df.columns:\n",
    "            dates = pd.to_datetime(df['JOURNAL_DATE'], errors='coerce')\n",
    "            features['month'] = dates.dt.month.fillna(0).astype(int)\n",
    "            features['day_of_week'] = dates.dt.dayofweek.fillna(0).astype(int)\n",
    "            features['day_of_month'] = dates.dt.day.fillna(0).astype(int)\n",
    "            features['quarter'] = dates.dt.quarter.fillna(0).astype(int)\n",
    "            features['is_weekend'] = dates.dt.dayofweek.isin([5, 6]).astype(int)\n",
    "            features['is_month_end'] = (dates.dt.day >= 25).astype(int)\n",
    "            features['is_friday'] = (dates.dt.dayofweek == 4).astype(int)\n",
    "            features['is_december'] = (dates.dt.month == 12).astype(int)\n",
    "            features['is_fbt_q4'] = dates.dt.month.isin([1, 2, 3]).astype(int)\n",
    "        \n",
    "        return features\n",
    "    \n",
    "    def extract_account_features(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        features = pd.DataFrame(index=df.index)\n",
    "        \n",
    "        if 'ACCOUNT_CODE' in df.columns:\n",
    "            code = df['ACCOUNT_CODE'].fillna(0).astype(int).astype(str)\n",
    "            features['account_code'] = code\n",
    "            features['account_group'] = code.str[:4]\n",
    "            \n",
    "            # Known entertainment-related accounts\n",
    "            features['is_entertainment_account'] = df['ACCOUNT_CODE'].isin([5130006000]).astype(int)\n",
    "            features['is_meals_account'] = df['ACCOUNT_CODE'].isin([5130004000]).astype(int)\n",
    "            features['is_travel_account'] = df['ACCOUNT_CODE'].isin([5130001000, 5130002000, 5130003000, 5130005000]).astype(int)\n",
    "        \n",
    "        return features\n",
    "    \n",
    "    def extract_text_features(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        features = pd.DataFrame(index=df.index)\n",
    "        combined = self.create_combined_text(df)\n",
    "        \n",
    "        # Length features\n",
    "        features['text_length'] = combined.str.len()\n",
    "        features['word_count'] = combined.str.split().str.len()\n",
    "        \n",
    "        # Keyword features\n",
    "        keyword_groups = {\n",
    "            'entertainment': ['dinner', 'lunch', 'breakfast', 'meal', 'restaurant',\n",
    "                              'catering', 'entertainment', 'party', 'celebration',\n",
    "                              'christmas', 'xmas', 'farewell', 'function', 'drinks',\n",
    "                              'alcohol', 'wine', 'beer', 'event'],\n",
    "            'travel': ['flight', 'airfare', 'taxi', 'uber', 'accommodation', 'hotel',\n",
    "                       'travel', 'airport', 'parking', 'qantas', 'virgin', 'cabcharge'],\n",
    "            'training': ['training', 'conference', 'seminar', 'workshop', 'course'],\n",
    "            'recreation': ['team building', 'teambuilding', 'fun day', 'bowling',\n",
    "                           'golf', 'sailing', 'escape room', 'activity'],\n",
    "            'client': ['client', 'customer', 'partner', 'stakeholder', 'external'],\n",
    "            'staff': ['staff', 'employee', 'team', 'internal', 'offsite']\n",
    "        }\n",
    "        \n",
    "        for group, keywords in keyword_groups.items():\n",
    "            pattern = '|'.join(keywords)\n",
    "            features[f'has_{group}_kw'] = combined.str.contains(pattern, na=False).astype(int)\n",
    "        \n",
    "        return features\n",
    "    \n",
    "    def extract_location_features(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        print(\"Extracting location features...\")\n",
    "        combined = self.create_combined_text(df)\n",
    "        \n",
    "        location_features = []\n",
    "        for i, text in enumerate(combined):\n",
    "            if i % 5000 == 0:\n",
    "                print(f\"  Processing row {i}/{len(combined)}\")\n",
    "            features = self.location_extractor.extract_location_features(text)\n",
    "            location_features.append(features)\n",
    "        \n",
    "        loc_df = pd.DataFrame(location_features, index=df.index)\n",
    "        print(f\"  Done. Found locations in {(loc_df['locations_found'] > 0).sum()} rows\")\n",
    "        return loc_df\n",
    "    \n",
    "    def engineer_all_features(self, df: pd.DataFrame) -> Tuple[pd.Series, pd.DataFrame]:\n",
    "        print(\"\\nEngineering features...\")\n",
    "        \n",
    "        combined_text = self.create_combined_text(df)\n",
    "        print(f\"  Combined text created\")\n",
    "        \n",
    "        amount_feat = self.extract_amount_features(df)\n",
    "        print(f\"  Amount features: {amount_feat.shape[1]}\")\n",
    "        \n",
    "        date_feat = self.extract_date_features(df)\n",
    "        print(f\"  Date features: {date_feat.shape[1]}\")\n",
    "        \n",
    "        account_feat = self.extract_account_features(df)\n",
    "        print(f\"  Account features: {account_feat.shape[1]}\")\n",
    "        \n",
    "        text_feat = self.extract_text_features(df)\n",
    "        print(f\"  Text features: {text_feat.shape[1]}\")\n",
    "        \n",
    "        location_feat = self.extract_location_features(df)\n",
    "        print(f\"  Location features: {location_feat.shape[1]}\")\n",
    "        \n",
    "        # Combine all features\n",
    "        all_features = pd.concat([\n",
    "            amount_feat, date_feat, account_feat, text_feat, location_feat\n",
    "        ], axis=1)\n",
    "        \n",
    "        print(f\"\\nTotal features: {all_features.shape[1]}\")\n",
    "        return combined_text, all_features"
   ],
   "outputs": [],
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-05T08:20:34.625552Z",
     "start_time": "2025-12-05T08:20:34.598442Z"
    }
   },
   "source": [
    "# Engineer features\n",
    "feature_engineer = FeatureEngineer()\n",
    "labeled_text, labeled_features = feature_engineer.engineer_all_features(labeled_data_clean)\n",
    "labeled_data_clean['combined_text'] = labeled_text"
   ],
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'labeled_data_clean' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[11], line 3\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m# Engineer features\u001B[39;00m\n\u001B[0;32m      2\u001B[0m feature_engineer \u001B[38;5;241m=\u001B[39m FeatureEngineer()\n\u001B[1;32m----> 3\u001B[0m labeled_text, labeled_features \u001B[38;5;241m=\u001B[39m feature_engineer\u001B[38;5;241m.\u001B[39mengineer_all_features(\u001B[43mlabeled_data_clean\u001B[49m)\n\u001B[0;32m      4\u001B[0m labeled_data_clean[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcombined_text\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m labeled_text\n",
      "\u001B[1;31mNameError\u001B[0m: name 'labeled_data_clean' is not defined"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview features\n",
    "print(\"\\nFeature Preview:\")\n",
    "print(labeled_features.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Location statistics\n",
    "print(\"\\nLocation Statistics:\")\n",
    "print(f\"Rows with locations: {(labeled_features['locations_found'] > 0).sum()}\")\n",
    "print(f\"\\nTravel Category Distribution:\")\n",
    "print(labeled_features['travel_category'].value_counts())\n",
    "print(f\"\\nDistance Statistics (km):\")\n",
    "print(labeled_features['distance_from_ref_km'].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Prepare Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_training_data(df: pd.DataFrame, features: pd.DataFrame, \n",
    "                          text_col: str = 'combined_text'):\n",
    "    \"\"\"\n",
    "    Prepare data for supervised training.\n",
    "    \"\"\"\n",
    "    # Find label column\n",
    "    label_col = 'me_label' if 'me_label' in df.columns else 'ME_LABEL'\n",
    "    \n",
    "    # Filter valid labels\n",
    "    valid_mask = df[label_col].isin(['Y', 'N'])\n",
    "    df_valid = df[valid_mask].copy()\n",
    "    features_valid = features[valid_mask].copy()\n",
    "    \n",
    "    print(f\"Valid samples: {len(df_valid)}\")\n",
    "    print(f\"Label distribution:\\n{df_valid[label_col].value_counts()}\")\n",
    "    \n",
    "    # Prepare data\n",
    "    X_text = df_valid[text_col].values\n",
    "    X_features = features_valid.copy()\n",
    "    y = df_valid[label_col].values\n",
    "    \n",
    "    # Encode labels\n",
    "    label_encoder = LabelEncoder()\n",
    "    y_encoded = label_encoder.fit_transform(y)\n",
    "    \n",
    "    print(f\"Classes: {label_encoder.classes_}\")\n",
    "    \n",
    "    return X_text, X_features, y_encoded, label_encoder, df_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_text, X_features, y, label_encoder, train_df = prepare_training_data(\n",
    "    labeled_data_clean, labeled_features\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data\n",
    "X_text_train, X_text_test, X_feat_train, X_feat_test, y_train, y_test = train_test_split(\n",
    "    X_text, X_features, y,\n",
    "    test_size=CONFIG['test_size'],\n",
    "    random_state=CONFIG['random_state'],\n",
    "    stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Training: {len(X_text_train)}\")\n",
    "print(f\"Test: {len(X_text_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Text Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF Vectorizer\n",
    "tfidf = TfidfVectorizer(\n",
    "    max_features=CONFIG['max_features'],\n",
    "    ngram_range=CONFIG['ngram_range'],\n",
    "    min_df=CONFIG['min_df'],\n",
    "    max_df=CONFIG['max_df'],\n",
    "    sublinear_tf=True,\n",
    "    strip_accents='unicode',\n",
    "    lowercase=True\n",
    ")\n",
    "\n",
    "X_tfidf_train = tfidf.fit_transform(X_text_train)\n",
    "X_tfidf_test = tfidf.transform(X_text_test)\n",
    "\n",
    "print(f\"TF-IDF shape: {X_tfidf_train.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare numerical features\n",
    "numerical_cols = X_feat_train.select_dtypes(include=[np.number]).columns.tolist()\n",
    "categorical_cols = ['amount_bucket', 'travel_category', 'account_code', 'account_group']\n",
    "categorical_cols = [c for c in categorical_cols if c in X_feat_train.columns]\n",
    "\n",
    "print(f\"Numerical features: {len(numerical_cols)}\")\n",
    "print(f\"Categorical features: {len(categorical_cols)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale numerical features\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X_num_train = X_feat_train[numerical_cols].fillna(0).values\n",
    "X_num_test = X_feat_test[numerical_cols].fillna(0).values\n",
    "\n",
    "X_num_train_scaled = scaler.fit_transform(X_num_train)\n",
    "X_num_test_scaled = scaler.transform(X_num_test)\n",
    "\n",
    "# Encode categorical features\n",
    "if categorical_cols:\n",
    "    cat_encoder = OneHotEncoder(sparse_output=True, handle_unknown='ignore')\n",
    "    X_cat_train = cat_encoder.fit_transform(X_feat_train[categorical_cols].fillna('unknown'))\n",
    "    X_cat_test = cat_encoder.transform(X_feat_test[categorical_cols].fillna('unknown'))\n",
    "else:\n",
    "    X_cat_train = None\n",
    "    X_cat_test = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all features\n",
    "from scipy import sparse\n",
    "\n",
    "feature_matrices_train = [X_tfidf_train, sparse.csr_matrix(X_num_train_scaled)]\n",
    "feature_matrices_test = [X_tfidf_test, sparse.csr_matrix(X_num_test_scaled)]\n",
    "\n",
    "if X_cat_train is not None:\n",
    "    feature_matrices_train.append(X_cat_train)\n",
    "    feature_matrices_test.append(X_cat_test)\n",
    "\n",
    "X_combined_train = sparse.hstack(feature_matrices_train)\n",
    "X_combined_test = sparse.hstack(feature_matrices_test)\n",
    "\n",
    "print(f\"Combined feature matrix: {X_combined_train.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define models\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(\n",
    "        max_iter=1000, random_state=42, class_weight='balanced', n_jobs=-1\n",
    "    ),\n",
    "    'Complement NB': ComplementNB(alpha=0.1),\n",
    "    'Linear SVC': CalibratedClassifierCV(\n",
    "        LinearSVC(max_iter=2000, random_state=42, class_weight='balanced')\n",
    "    ),\n",
    "    'SGD Classifier': SGDClassifier(\n",
    "        loss='modified_huber', max_iter=1000, random_state=42, \n",
    "        class_weight='balanced', n_jobs=-1\n",
    "    ),\n",
    "    'Random Forest': RandomForestClassifier(\n",
    "        n_estimators=200, max_depth=20, random_state=42,\n",
    "        class_weight='balanced', n_jobs=-1\n",
    "    ),\n",
    "    'Gradient Boosting': GradientBoostingClassifier(\n",
    "        n_estimators=100, max_depth=5, random_state=42\n",
    "    ),\n",
    "    'Extra Trees': ExtraTreesClassifier(\n",
    "        n_estimators=200, max_depth=20, random_state=42,\n",
    "        class_weight='balanced', n_jobs=-1\n",
    "    )\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and evaluate models\n",
    "results = []\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"\\nTraining {name}...\")\n",
    "    \n",
    "    # Cross-validation\n",
    "    cv = StratifiedKFold(n_splits=CONFIG['cv_folds'], shuffle=True, random_state=42)\n",
    "    cv_scores = cross_val_score(model, X_combined_train, y_train, cv=cv, scoring='f1')\n",
    "    \n",
    "    # Train\n",
    "    model.fit(X_combined_train, y_train)\n",
    "    \n",
    "    # Predict\n",
    "    y_pred = model.predict(X_combined_test)\n",
    "    \n",
    "    # Metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    \n",
    "    results.append({\n",
    "        'Model': name,\n",
    "        'CV F1 Mean': cv_scores.mean(),\n",
    "        'CV F1 Std': cv_scores.std(),\n",
    "        'Test Accuracy': accuracy,\n",
    "        'Test F1': f1\n",
    "    })\n",
    "    \n",
    "    print(f\"  CV F1: {cv_scores.mean():.4f} (+/- {cv_scores.std():.4f})\")\n",
    "    print(f\"  Test F1: {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results comparison\n",
    "results_df = pd.DataFrame(results).sort_values('Test F1', ascending=False)\n",
    "print(\"\\nModel Comparison:\")\n",
    "print(results_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize results\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "x = np.arange(len(results_df))\n",
    "width = 0.35\n",
    "\n",
    "ax.bar(x - width/2, results_df['CV F1 Mean'], width, label='CV F1', alpha=0.8)\n",
    "ax.bar(x + width/2, results_df['Test F1'], width, label='Test F1', alpha=0.8)\n",
    "\n",
    "ax.set_ylabel('F1 Score')\n",
    "ax.set_title('Model Comparison')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(results_df['Model'], rotation=45, ha='right')\n",
    "ax.legend()\n",
    "ax.set_ylim(0, 1)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Best Model Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select best model\n",
    "best_name = results_df.iloc[0]['Model']\n",
    "best_model = models[best_name]\n",
    "\n",
    "print(f\"Best Model: {best_name}\")\n",
    "\n",
    "y_pred = best_model.predict(X_combined_test)\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred, target_names=label_encoder.classes_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=label_encoder.classes_)\n",
    "disp.plot(ax=ax, cmap='Blues', values_format='d')\n",
    "ax.set_title(f'Confusion Matrix - {best_name}')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Unsupervised Clustering (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CONFIG.get('enable_clustering', False):\n",
    "    print(\"Running clustering analysis...\")\n",
    "    \n",
    "    # Reduce dimensionality for clustering\n",
    "    svd = TruncatedSVD(n_components=100, random_state=42)\n",
    "    X_reduced = svd.fit_transform(X_combined_train)\n",
    "    \n",
    "    # K-Means clustering\n",
    "    n_clusters = CONFIG['n_clusters']\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)\n",
    "    cluster_labels = kmeans.fit_predict(X_reduced)\n",
    "    \n",
    "    # Evaluate clustering\n",
    "    silhouette = silhouette_score(X_reduced, cluster_labels)\n",
    "    calinski = calinski_harabasz_score(X_reduced, cluster_labels)\n",
    "    \n",
    "    print(f\"\\nClustering Metrics:\")\n",
    "    print(f\"  Silhouette Score: {silhouette:.4f}\")\n",
    "    print(f\"  Calinski-Harabasz Score: {calinski:.2f}\")\n",
    "    \n",
    "    # Cluster distribution\n",
    "    print(f\"\\nCluster Distribution:\")\n",
    "    print(pd.Series(cluster_labels).value_counts().sort_index())\n",
    "    \n",
    "    # Visualize with t-SNE\n",
    "    print(\"\\nGenerating t-SNE visualization...\")\n",
    "    tsne = TSNE(n_components=2, random_state=42, perplexity=30)\n",
    "    X_tsne = tsne.fit_transform(X_reduced[:2000])  # Limit for speed\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "    \n",
    "    # Clusters\n",
    "    scatter = axes[0].scatter(X_tsne[:, 0], X_tsne[:, 1], c=cluster_labels[:2000], \n",
    "                              cmap='tab10', alpha=0.6, s=10)\n",
    "    axes[0].set_title('t-SNE: Cluster Labels')\n",
    "    plt.colorbar(scatter, ax=axes[0])\n",
    "    \n",
    "    # True labels\n",
    "    scatter = axes[1].scatter(X_tsne[:, 0], X_tsne[:, 1], c=y_train[:2000],\n",
    "                              cmap='RdYlGn', alpha=0.6, s=10)\n",
    "    axes[1].set_title('t-SNE: True Labels (Y=Red, N=Green)')\n",
    "    plt.colorbar(scatter, ax=axes[1])\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Clustering disabled. Set CONFIG['enable_clustering'] = True to enable.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_features(model, tfidf_vectorizer, n=30):\n",
    "    \"\"\"Get top features from model.\"\"\"\n",
    "    feature_names = list(tfidf_vectorizer.get_feature_names_out())\n",
    "    feature_names.extend(numerical_cols)\n",
    "    \n",
    "    if hasattr(model, 'coef_'):\n",
    "        coef = model.coef_[0] if len(model.coef_.shape) > 1 else model.coef_\n",
    "        coef = coef[:len(feature_names)]  # Limit to named features\n",
    "        \n",
    "        # Top positive (ME = Y)\n",
    "        top_pos_idx = np.argsort(coef)[-n:]\n",
    "        top_pos = [(feature_names[i], coef[i]) for i in reversed(top_pos_idx) if i < len(feature_names)]\n",
    "        \n",
    "        # Top negative (ME = N)\n",
    "        top_neg_idx = np.argsort(coef)[:n]\n",
    "        top_neg = [(feature_names[i], coef[i]) for i in top_neg_idx if i < len(feature_names)]\n",
    "        \n",
    "        return top_pos, top_neg\n",
    "    \n",
    "    elif hasattr(model, 'feature_importances_'):\n",
    "        importance = model.feature_importances_[:len(feature_names)]\n",
    "        top_idx = np.argsort(importance)[-n:]\n",
    "        top = [(feature_names[i], importance[i]) for i in reversed(top_idx) if i < len(feature_names)]\n",
    "        return top, []\n",
    "    \n",
    "    return [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature importance for best model\n",
    "if hasattr(best_model, 'coef_') or hasattr(best_model, 'feature_importances_'):\n",
    "    top_pos, top_neg = get_top_features(best_model, tfidf)\n",
    "    \n",
    "    print(\"Top Features for ME = Y (FBT Subject):\")\n",
    "    print(\"-\" * 50)\n",
    "    for feat, score in top_pos[:20]:\n",
    "        print(f\"  {feat:35s} {score:+.4f}\")\n",
    "    \n",
    "    if top_neg:\n",
    "        print(\"\\nTop Features for ME = N (Not FBT Subject):\")\n",
    "        print(\"-\" * 50)\n",
    "        for feat, score in top_neg[:20]:\n",
    "            print(f\"  {feat:35s} {score:+.4f}\")\n",
    "else:\n",
    "    print(\"Feature importance not available for this model type.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid search for Logistic Regression\n",
    "param_grid = {\n",
    "    'C': [0.01, 0.1, 1, 10],\n",
    "    'penalty': ['l1', 'l2'],\n",
    "    'solver': ['saga']\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    LogisticRegression(max_iter=1000, random_state=42, class_weight='balanced'),\n",
    "    param_grid,\n",
    "    cv=5,\n",
    "    scoring='f1',\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"Running GridSearchCV...\")\n",
    "grid_search.fit(X_combined_train, y_train)\n",
    "\n",
    "print(f\"\\nBest parameters: {grid_search.best_params_}\")\n",
    "print(f\"Best CV F1: {grid_search.best_score_:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate tuned model\n",
    "tuned_model = grid_search.best_estimator_\n",
    "y_pred_tuned = tuned_model.predict(X_combined_test)\n",
    "\n",
    "print(\"\\nTuned Model Results:\")\n",
    "print(classification_report(y_test, y_pred_tuned, target_names=label_encoder.classes_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15. Ensemble Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create ensemble\n",
    "ensemble = VotingClassifier(\n",
    "    estimators=[\n",
    "        ('lr', LogisticRegression(max_iter=1000, random_state=42, class_weight='balanced')),\n",
    "        ('nb', ComplementNB(alpha=0.1)),\n",
    "        ('svc', CalibratedClassifierCV(LinearSVC(max_iter=2000, random_state=42, class_weight='balanced'))),\n",
    "        ('rf', RandomForestClassifier(n_estimators=100, max_depth=15, random_state=42, n_jobs=-1))\n",
    "    ],\n",
    "    voting='soft'\n",
    ")\n",
    "\n",
    "print(\"Training ensemble...\")\n",
    "ensemble.fit(X_combined_train, y_train)\n",
    "\n",
    "y_pred_ensemble = ensemble.predict(X_combined_test)\n",
    "print(\"\\nEnsemble Results:\")\n",
    "print(classification_report(y_test, y_pred_ensemble, target_names=label_encoder.classes_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16. Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save all components\n",
    "model_package = {\n",
    "    'model': ensemble,\n",
    "    'tfidf': tfidf,\n",
    "    'scaler': scaler,\n",
    "    'cat_encoder': cat_encoder if categorical_cols else None,\n",
    "    'label_encoder': label_encoder,\n",
    "    'numerical_cols': numerical_cols,\n",
    "    'categorical_cols': categorical_cols,\n",
    "    'config': CONFIG\n",
    "}\n",
    "\n",
    "joblib.dump(model_package, CONFIG['model_output'])\n",
    "print(f\"Model saved to: {CONFIG['model_output']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 17. Inference Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_fbt(texts: List[str], features_df: pd.DataFrame = None,\n",
    "                model_path: str = CONFIG['model_output']) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Predict FBT classification for new expenses.\n",
    "    \"\"\"\n",
    "    # Load model\n",
    "    pkg = joblib.load(model_path)\n",
    "    \n",
    "    # Transform text\n",
    "    X_tfidf = pkg['tfidf'].transform(texts)\n",
    "    \n",
    "    # Handle features\n",
    "    if features_df is not None:\n",
    "        X_num = features_df[pkg['numerical_cols']].fillna(0).values\n",
    "        X_num_scaled = pkg['scaler'].transform(X_num)\n",
    "        \n",
    "        if pkg['cat_encoder']:\n",
    "            X_cat = pkg['cat_encoder'].transform(features_df[pkg['categorical_cols']].fillna('unknown'))\n",
    "            X_combined = sparse.hstack([X_tfidf, sparse.csr_matrix(X_num_scaled), X_cat])\n",
    "        else:\n",
    "            X_combined = sparse.hstack([X_tfidf, sparse.csr_matrix(X_num_scaled)])\n",
    "    else:\n",
    "        X_combined = X_tfidf\n",
    "    \n",
    "    # Predict\n",
    "    predictions = pkg['model'].predict(X_combined)\n",
    "    labels = pkg['label_encoder'].inverse_transform(predictions)\n",
    "    \n",
    "    # Probabilities\n",
    "    if hasattr(pkg['model'], 'predict_proba'):\n",
    "        probs = pkg['model'].predict_proba(X_combined)\n",
    "        prob_y = probs[:, 1]  # Probability of Y (ME)\n",
    "    else:\n",
    "        prob_y = np.nan\n",
    "    \n",
    "    return pd.DataFrame({\n",
    "        'text': texts,\n",
    "        'prediction': labels,\n",
    "        'prob_ME': prob_y\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test inference\n",
    "test_texts = [\n",
    "    \"client dinner at restaurant melbourne 5 partners\",\n",
    "    \"taxi to airport business trip brisbane\",\n",
    "    \"team building activity bowling sydney\",\n",
    "    \"training seminar registration fee\",\n",
    "    \"christmas party catering staff 50 people\",\n",
    "    \"flight to roma client visit farm\",\n",
    "    \"lunch meeting with client local cafe\"\n",
    "]\n",
    "\n",
    "predictions = predict_fbt(test_texts)\n",
    "print(\"\\nSample Predictions:\")\n",
    "print(predictions.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 18. Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"PIPELINE SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nData:\")\n",
    "print(f\"  Raw samples: {len(raw_data_clean)}\")\n",
    "print(f\"  Labeled samples: {len(train_df)}\")\n",
    "print(f\"  Training samples: {len(X_text_train)}\")\n",
    "print(f\"  Test samples: {len(X_text_test)}\")\n",
    "print(f\"\\nFeatures:\")\n",
    "print(f\"  TF-IDF features: {X_tfidf_train.shape[1]}\")\n",
    "print(f\"  Numerical features: {len(numerical_cols)}\")\n",
    "print(f\"  Categorical features: {len(categorical_cols)}\")\n",
    "print(f\"  Total features: {X_combined_train.shape[1]}\")\n",
    "print(f\"\\nBest Model: {best_name}\")\n",
    "print(f\"  Test F1: {results_df.iloc[0]['Test F1']:.4f}\")\n",
    "print(f\"\\nLocation Features:\")\n",
    "print(f\"  Locations database: {len(AUSTRALIAN_LOCATIONS)} entries\")\n",
    "print(f\"  Samples with locations: {(labeled_features['locations_found'] > 0).sum()}\")\n",
    "print(f\"\\nSaved:\")\n",
    "print(f\"  Model: {CONFIG['model_output']}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
