{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FBT Classification Pipeline\n",
    "## Fringe Benefit Tax: Yes or No?\n",
    "\n",
    "**Goal:** Classify if an expense is subject to Fringe Benefit Tax\n",
    "\n",
    "**Output:**\n",
    "- `fbt_label`: Y (taxable) / N (not taxable)\n",
    "- `category`: Detailed category for explanation\n",
    "- `location`: Extracted location info\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG = {\n",
    "    'raw_data_path': 'data/data _raw_2024-25.xlsx',\n",
    "    'wp_files': [\n",
    "        'data/WP_1_Apr_to_Dec_24_FBT_ent_acc.xlsx',\n",
    "        'data/WP_2_Apr_to_Dec_24_FBT_ent_acc.xlsx',\n",
    "        'data/WP_3_Jan_to_Mar_25_FBT_ent_acc.xlsx',\n",
    "        'data/WP_4_Jan_to_Mar_25_FBT_ent_acc.xlsx'\n",
    "    ],\n",
    "    'reference_location': {'lat': -33.8688, 'lon': 151.2093, 'name': 'Sydney CBD'},\n",
    "    'test_size': 0.2,\n",
    "    'random_state': 42,\n",
    "    'cv_folds': 5,\n",
    "    'max_features': 5000,\n",
    "    'ngram_range': (1, 3),\n",
    "    'min_df': 2,\n",
    "    'max_df': 0.95,\n",
    "    'model_output': 'fbt_model.joblib',\n",
    "    'predictions_output': 'fbt_predictions.csv'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import math\n",
    "import warnings\n",
    "from typing import List, Dict, Optional\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, VotingClassifier, GradientBoostingClassifier\n",
    "from sklearn.naive_bayes import ComplementNB\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.metrics import classification_report, confusion_matrix, f1_score, ConfusionMatrixDisplay, accuracy_score\n",
    "from scipy import sparse\n",
    "import joblib\n",
    "\n",
    "print(\"Imports successful\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. FBT Category Mapping\n",
    "\n",
    "**Fringe Benefit = Y (Taxable):**\n",
    "- Meal Entertainment (ME)\n",
    "- Celebrations (Xmas, farewell, birthday)\n",
    "- Alcohol\n",
    "- Travel FOR entertainment\n",
    "- Lodging FOR entertainment\n",
    "\n",
    "**Fringe Benefit = N (Not Taxable):**\n",
    "- Business travel meals\n",
    "- Business expenses\n",
    "- Training/conference\n",
    "- 58P Recreation (non-deductible but no FBT)\n",
    "- Office consumables\n",
    "- Pure business travel/lodging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# FBT CLASSIFICATION RULES\n",
    "# =============================================================================\n",
    "\n",
    "# Categories that ARE subject to FBT\n",
    "FBT_YES_CATEGORIES = [\n",
    "    'MEAL_ENTERTAINMENT',      # ME - client dinners, staff lunches\n",
    "    'CELEBRATION',             # Xmas party, farewell, birthday\n",
    "    'ALCOHOL',                 # Drinks, wine, beer\n",
    "    'TRAVEL_FOR_ENTERTAINMENT', # Travel connected to entertainment\n",
    "    'LODGING_FOR_ENTERTAINMENT' # Accommodation for entertainment\n",
    "]\n",
    "\n",
    "# Categories NOT subject to FBT\n",
    "FBT_NO_CATEGORIES = [\n",
    "    'BUSINESS_TRAVEL_MEAL',    # Meal whilst travelling for business\n",
    "    'BUSINESS_EXPENSE',        # General business expense\n",
    "    'TRAINING',                # Conference, seminar, workshop\n",
    "    'RECREATION_58P',          # 58P recreational (no FBT but non-deductible)\n",
    "    'CLIENT_EVENT',            # Client-focused events (field days)\n",
    "    'SUSTENANCE',              # Morning tea, working lunch on premises\n",
    "    'BUSINESS_TRAVEL',         # Pure business travel\n",
    "    'BUSINESS_LODGING'         # Pure business accommodation\n",
    "]\n",
    "\n",
    "# Mapping rules: Tax Description keywords -> Category\n",
    "CATEGORY_RULES = {\n",
    "    # FBT = YES\n",
    "    'MEAL_ENTERTAINMENT': [\n",
    "        'me', 'meal entertainment', 'dinner', 'lunch', 'breakfast', \n",
    "        'restaurant', 'catering', 'me - no workplace'\n",
    "    ],\n",
    "    'CELEBRATION': [\n",
    "        'xmas', 'christmas', 'party', 'farewell', 'celebration', \n",
    "        'birthday', 'bd cake', 'welcome', 'anniversary', 'end of year'\n",
    "    ],\n",
    "    'ALCOHOL': [\n",
    "        'alcohol', 'drinks', 'wine', 'beer', 'liquor'\n",
    "    ],\n",
    "    'TRAVEL_FOR_ENTERTAINMENT': [\n",
    "        'travel connected to entertainment', 'travel associated with entertainment',\n",
    "        'travel related to rabo me', 'travel associated with me',\n",
    "        'travel for entertainment', 'travel associated with dinner',\n",
    "        'travel to entertainment'\n",
    "    ],\n",
    "    'LODGING_FOR_ENTERTAINMENT': [\n",
    "        'accommodation for entertainment', 'lodging for entertainment',\n",
    "        'hotel for entertainment', 'stay for entertainment'\n",
    "    ],\n",
    "    \n",
    "    # FBT = NO\n",
    "    'BUSINESS_TRAVEL_MEAL': [\n",
    "        'accept travel meal', 'travel meal', 'meal whilst travel',\n",
    "        'business travel food', 'accept business travel food',\n",
    "        'ok - accept dinner on travel'\n",
    "    ],\n",
    "    'BUSINESS_EXPENSE': [\n",
    "        'business expense', 'ok', 'accept', 'no fbt', 'tax deductible',\n",
    "        'office consumable', 'deductible', 'ok - business',\n",
    "        'accept predominately business', 'business related'\n",
    "    ],\n",
    "    'TRAINING': [\n",
    "        'training', 'conference', 'seminar', 'workshop', 'course',\n",
    "        'accept seminar', 'accept seminar food'\n",
    "    ],\n",
    "    'RECREATION_58P': [\n",
    "        '58p', 'recreational', 'team building', 'bowling', 'golf', \n",
    "        'sailing', 'fun day', 'escape room'\n",
    "    ],\n",
    "    'CLIENT_EVENT': [\n",
    "        'client event', 'client focused', 'field day', 'client visit',\n",
    "        'travel - predominant purpose client visit'\n",
    "    ],\n",
    "    'SUSTENANCE': [\n",
    "        'sustenance', 'morning tea', 'on premises', 'light lunch', 'working lunch'\n",
    "    ],\n",
    "    'BUSINESS_TRAVEL': [\n",
    "        'accept business travel', 'accept work travel', 'business travel - accept'\n",
    "    ],\n",
    "    'BUSINESS_LODGING': [\n",
    "        'accept business lodging', 'business accommodation'\n",
    "    ]\n",
    "}\n",
    "\n",
    "\n",
    "def map_to_category(tax_desc: str, me_label: str = None) -> str:\n",
    "    \"\"\"\n",
    "    Map Tax Description to category.\n",
    "    Also considers ME? label if available.\n",
    "    \"\"\"\n",
    "    if pd.isna(tax_desc) or not str(tax_desc).strip():\n",
    "        # No tax description - check ME label\n",
    "        if me_label and str(me_label).upper() == 'Y':\n",
    "            return 'MEAL_ENTERTAINMENT'\n",
    "        return 'UNLABELED'\n",
    "    \n",
    "    tax_lower = str(tax_desc).lower().strip()\n",
    "    \n",
    "    # Exact match for 'me'\n",
    "    if tax_lower == 'me':\n",
    "        return 'MEAL_ENTERTAINMENT'\n",
    "    \n",
    "    # Check each category\n",
    "    for category, keywords in CATEGORY_RULES.items():\n",
    "        for kw in keywords:\n",
    "            if kw in tax_lower:\n",
    "                return category\n",
    "    \n",
    "    # Check ME label as fallback\n",
    "    if me_label and str(me_label).upper() == 'Y':\n",
    "        return 'MEAL_ENTERTAINMENT'\n",
    "    \n",
    "    return 'OTHER'\n",
    "\n",
    "\n",
    "def get_fbt_label(category: str) -> str:\n",
    "    \"\"\"\n",
    "    Get FBT Y/N from category.\n",
    "    \"\"\"\n",
    "    if category in FBT_YES_CATEGORIES:\n",
    "        return 'Y'\n",
    "    elif category in FBT_NO_CATEGORIES:\n",
    "        return 'N'\n",
    "    return 'UNKNOWN'\n",
    "\n",
    "\n",
    "print(\"FBT mapping defined\")\n",
    "print(f\"\\nFBT = Y categories: {FBT_YES_CATEGORIES}\")\n",
    "print(f\"\\nFBT = N categories: {FBT_NO_CATEGORIES}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Location Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOCATIONS = {\n",
    "    # Major Cities\n",
    "    'sydney': {'lat': -33.8688, 'lon': 151.2093, 'state': 'NSW', 'type': 'city'},\n",
    "    'melbourne': {'lat': -37.8136, 'lon': 144.9631, 'state': 'VIC', 'type': 'city'},\n",
    "    'brisbane': {'lat': -27.4698, 'lon': 153.0251, 'state': 'QLD', 'type': 'city'},\n",
    "    'perth': {'lat': -31.9505, 'lon': 115.8605, 'state': 'WA', 'type': 'city'},\n",
    "    'adelaide': {'lat': -34.9285, 'lon': 138.6007, 'state': 'SA', 'type': 'city'},\n",
    "    'hobart': {'lat': -42.8821, 'lon': 147.3272, 'state': 'TAS', 'type': 'city'},\n",
    "    'darwin': {'lat': -12.4634, 'lon': 130.8456, 'state': 'NT', 'type': 'city'},\n",
    "    'canberra': {'lat': -35.2809, 'lon': 149.1300, 'state': 'ACT', 'type': 'city'},\n",
    "    # Regional NSW\n",
    "    'dubbo': {'lat': -32.2569, 'lon': 148.6011, 'state': 'NSW', 'type': 'regional'},\n",
    "    'wagga wagga': {'lat': -35.1082, 'lon': 147.3598, 'state': 'NSW', 'type': 'regional'},\n",
    "    'wagga': {'lat': -35.1082, 'lon': 147.3598, 'state': 'NSW', 'type': 'regional'},\n",
    "    'tamworth': {'lat': -31.0830, 'lon': 150.9170, 'state': 'NSW', 'type': 'regional'},\n",
    "    'orange': {'lat': -33.2840, 'lon': 149.1004, 'state': 'NSW', 'type': 'regional'},\n",
    "    'bathurst': {'lat': -33.4190, 'lon': 149.5778, 'state': 'NSW', 'type': 'regional'},\n",
    "    'albury': {'lat': -36.0737, 'lon': 146.9135, 'state': 'NSW', 'type': 'regional'},\n",
    "    'moree': {'lat': -29.4640, 'lon': 149.8470, 'state': 'NSW', 'type': 'regional'},\n",
    "    'griffith': {'lat': -34.2890, 'lon': 146.0400, 'state': 'NSW', 'type': 'regional'},\n",
    "    'parkes': {'lat': -33.1370, 'lon': 148.1750, 'state': 'NSW', 'type': 'regional'},\n",
    "    'narrabri': {'lat': -30.3250, 'lon': 149.7830, 'state': 'NSW', 'type': 'regional'},\n",
    "    'newcastle': {'lat': -32.9283, 'lon': 151.7817, 'state': 'NSW', 'type': 'regional'},\n",
    "    'wollongong': {'lat': -34.4278, 'lon': 150.8931, 'state': 'NSW', 'type': 'regional'},\n",
    "    # Regional QLD\n",
    "    'roma': {'lat': -26.5700, 'lon': 148.7850, 'state': 'QLD', 'type': 'regional'},\n",
    "    'toowoomba': {'lat': -27.5598, 'lon': 151.9507, 'state': 'QLD', 'type': 'regional'},\n",
    "    'rockhampton': {'lat': -23.3791, 'lon': 150.5100, 'state': 'QLD', 'type': 'regional'},\n",
    "    'mackay': {'lat': -21.1411, 'lon': 149.1861, 'state': 'QLD', 'type': 'regional'},\n",
    "    'townsville': {'lat': -19.2590, 'lon': 146.8169, 'state': 'QLD', 'type': 'regional'},\n",
    "    'cairns': {'lat': -16.9186, 'lon': 145.7781, 'state': 'QLD', 'type': 'regional'},\n",
    "    'longreach': {'lat': -23.4420, 'lon': 144.2500, 'state': 'QLD', 'type': 'regional'},\n",
    "    'mount isa': {'lat': -20.7256, 'lon': 139.4927, 'state': 'QLD', 'type': 'regional'},\n",
    "    'emerald': {'lat': -23.5270, 'lon': 148.1640, 'state': 'QLD', 'type': 'regional'},\n",
    "    'dalby': {'lat': -27.1810, 'lon': 151.2650, 'state': 'QLD', 'type': 'regional'},\n",
    "    'goondiwindi': {'lat': -28.5470, 'lon': 150.3100, 'state': 'QLD', 'type': 'regional'},\n",
    "    'charleville': {'lat': -26.4030, 'lon': 146.2430, 'state': 'QLD', 'type': 'regional'},\n",
    "    'cloncurry': {'lat': -20.7050, 'lon': 140.5060, 'state': 'QLD', 'type': 'regional'},\n",
    "    'gold coast': {'lat': -28.0167, 'lon': 153.4000, 'state': 'QLD', 'type': 'regional'},\n",
    "    # Regional VIC\n",
    "    'geelong': {'lat': -38.1499, 'lon': 144.3617, 'state': 'VIC', 'type': 'regional'},\n",
    "    'ballarat': {'lat': -37.5622, 'lon': 143.8503, 'state': 'VIC', 'type': 'regional'},\n",
    "    'bendigo': {'lat': -36.7570, 'lon': 144.2794, 'state': 'VIC', 'type': 'regional'},\n",
    "    'shepparton': {'lat': -36.3833, 'lon': 145.4000, 'state': 'VIC', 'type': 'regional'},\n",
    "    'mildura': {'lat': -34.2087, 'lon': 142.1311, 'state': 'VIC', 'type': 'regional'},\n",
    "    'horsham': {'lat': -36.7117, 'lon': 142.2000, 'state': 'VIC', 'type': 'regional'},\n",
    "    # Regional SA/WA\n",
    "    'port lincoln': {'lat': -34.7333, 'lon': 135.8500, 'state': 'SA', 'type': 'regional'},\n",
    "    'port augusta': {'lat': -32.4936, 'lon': 137.7825, 'state': 'SA', 'type': 'regional'},\n",
    "    'geraldton': {'lat': -28.7775, 'lon': 114.6147, 'state': 'WA', 'type': 'regional'},\n",
    "    'kalgoorlie': {'lat': -30.7489, 'lon': 121.4658, 'state': 'WA', 'type': 'regional'},\n",
    "    'broome': {'lat': -17.9614, 'lon': 122.2359, 'state': 'WA', 'type': 'regional'},\n",
    "    'karratha': {'lat': -20.7361, 'lon': 116.8467, 'state': 'WA', 'type': 'regional'},\n",
    "    # International\n",
    "    'utrecht': {'lat': 52.0907, 'lon': 5.1214, 'state': 'NL', 'type': 'international'},\n",
    "    'amsterdam': {'lat': 52.3676, 'lon': 4.9041, 'state': 'NL', 'type': 'international'},\n",
    "    'singapore': {'lat': 1.3521, 'lon': 103.8198, 'state': 'SG', 'type': 'international'},\n",
    "    'hong kong': {'lat': 22.3193, 'lon': 114.1694, 'state': 'HK', 'type': 'international'},\n",
    "    'london': {'lat': 51.5074, 'lon': -0.1278, 'state': 'UK', 'type': 'international'},\n",
    "    'new zealand': {'lat': -40.9006, 'lon': 174.8860, 'state': 'NZ', 'type': 'international'},\n",
    "    'auckland': {'lat': -36.8509, 'lon': 174.7645, 'state': 'NZ', 'type': 'international'},\n",
    "    'tokyo': {'lat': 35.6762, 'lon': 139.6503, 'state': 'JP', 'type': 'international'},\n",
    "}\n",
    "\n",
    "class LocationExtractor:\n",
    "    def __init__(self, ref: Dict):\n",
    "        self.ref = ref\n",
    "        names = sorted(LOCATIONS.keys(), key=len, reverse=True)\n",
    "        self.pattern = re.compile(r'\\b(' + '|'.join(re.escape(n) for n in names) + r')\\b', re.I)\n",
    "    \n",
    "    def haversine(self, lat1, lon1, lat2, lon2):\n",
    "        R = 6371\n",
    "        lat1, lon1, lat2, lon2 = map(math.radians, [lat1, lon1, lat2, lon2])\n",
    "        dlat, dlon = lat2 - lat1, lon2 - lon1\n",
    "        a = math.sin(dlat/2)**2 + math.cos(lat1) * math.cos(lat2) * math.sin(dlon/2)**2\n",
    "        return R * 2 * math.atan2(math.sqrt(a), math.sqrt(1-a))\n",
    "    \n",
    "    def extract(self, text: str) -> Dict:\n",
    "        result = {\n",
    "            'location_name': '', 'location_state': '',\n",
    "            'has_location': 0, 'distance_km': 0, 'travel_hours': 0,\n",
    "            'is_international': 0, 'is_regional': 0, 'is_remote': 0\n",
    "        }\n",
    "        if pd.isna(text) or not text:\n",
    "            return result\n",
    "        \n",
    "        matches = self.pattern.findall(str(text).lower())\n",
    "        if not matches:\n",
    "            return result\n",
    "        \n",
    "        loc = matches[0].lower()\n",
    "        if loc not in LOCATIONS:\n",
    "            return result\n",
    "        \n",
    "        coords = LOCATIONS[loc]\n",
    "        distance = self.haversine(self.ref['lat'], self.ref['lon'], coords['lat'], coords['lon'])\n",
    "        travel = (distance / 800) + 2 if distance > 500 else distance / 80\n",
    "        \n",
    "        return {\n",
    "            'location_name': loc.title(),\n",
    "            'location_state': coords['state'],\n",
    "            'has_location': 1,\n",
    "            'distance_km': round(distance, 0),\n",
    "            'travel_hours': round(travel, 1),\n",
    "            'is_international': 1 if coords['type'] == 'international' else 0,\n",
    "            'is_regional': 1 if coords['type'] == 'regional' else 0,\n",
    "            'is_remote': 1 if distance > 500 else 0\n",
    "        }\n",
    "\n",
    "loc_extractor = LocationExtractor(CONFIG['reference_location'])\n",
    "print(f\"Loaded {len(LOCATIONS)} locations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_raw_data(filepath: str) -> pd.DataFrame:\n",
    "    print(f\"Loading: {filepath}\")\n",
    "    xl = pd.ExcelFile(filepath)\n",
    "    dfs = []\n",
    "    for sheet in xl.sheet_names:\n",
    "        df = pd.read_excel(filepath, sheet_name=sheet)\n",
    "        df['_sheet'] = sheet\n",
    "        dfs.append(df)\n",
    "        print(f\"  {sheet}: {len(df)} rows\")\n",
    "    combined = pd.concat(dfs, ignore_index=True)\n",
    "    print(f\"Total: {len(combined)} rows\")\n",
    "    return combined\n",
    "\n",
    "\n",
    "def parse_workpaper(filepath: str, sheet: str) -> Optional[pd.DataFrame]:\n",
    "    try:\n",
    "        df = pd.read_excel(filepath, sheet_name=sheet, header=None)\n",
    "        header_idx = None\n",
    "        for i, row in df.iterrows():\n",
    "            if 'BUSINESS_UNIT_CODE' in ' '.join([str(v).upper() for v in row.values if pd.notna(v)]):\n",
    "                header_idx = i\n",
    "                break\n",
    "        if header_idx is None:\n",
    "            return None\n",
    "        \n",
    "        headers = [str(h).strip() if pd.notna(h) else f'_col_{i}' for i, h in enumerate(df.iloc[header_idx])]\n",
    "        data = df.iloc[header_idx + 1:].copy()\n",
    "        data.columns = headers\n",
    "        \n",
    "        # Rename label columns\n",
    "        for col in headers:\n",
    "            cl = str(col).lower()\n",
    "            if 'tax' in cl and 'desc' in cl:\n",
    "                data = data.rename(columns={col: 'TAX_DESCRIPTION'})\n",
    "            elif 'me?' in cl:\n",
    "                data = data.rename(columns={col: 'ME_LABEL'})\n",
    "        \n",
    "        data['_sheet'] = sheet\n",
    "        data['_file'] = filepath.split('/')[-1]\n",
    "        return data\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "\n",
    "def load_labeled_data(wp_files: List[str]) -> pd.DataFrame:\n",
    "    all_dfs = []\n",
    "    for fp in wp_files:\n",
    "        print(f\"\\nProcessing: {fp}\")\n",
    "        try:\n",
    "            xl = pd.ExcelFile(fp)\n",
    "            for sheet in xl.sheet_names:\n",
    "                if 'summary' in sheet.lower() or 'trial balance' in sheet.lower():\n",
    "                    continue\n",
    "                df = parse_workpaper(fp, sheet)\n",
    "                if df is not None and len(df) > 0:\n",
    "                    all_dfs.append(df)\n",
    "                    print(f\"  {sheet}: {len(df)} rows\")\n",
    "        except Exception as e:\n",
    "            print(f\"  Error: {e}\")\n",
    "    \n",
    "    combined = pd.concat(all_dfs, ignore_index=True) if all_dfs else pd.DataFrame()\n",
    "    print(f\"\\nTotal: {len(combined)} rows\")\n",
    "    return combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = load_raw_data(CONFIG['raw_data_path'])\n",
    "labeled_data = load_labeled_data(CONFIG['wp_files'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    # Remove junk columns\n",
    "    df = df.loc[:, ~df.columns.str.contains('^Unnamed|^_col_', case=False, na=False)]\n",
    "    \n",
    "    # Standardize column names\n",
    "    new_cols = []\n",
    "    for c in df.columns:\n",
    "        c_clean = re.sub(r'[^a-zA-Z0-9_]', '', re.sub(r'\\s+', '_', str(c).strip())).upper()\n",
    "        new_cols.append(c_clean)\n",
    "    df.columns = new_cols\n",
    "    \n",
    "    # Clean text columns\n",
    "    text_cols = ['PURPOSE', 'CHARGE_DESCRIPTION', 'LINE_DESCR', 'DESCRIPTION', \n",
    "                 'INVOICE_DESCR', 'VENDOR_NAME', 'TAX_DESCRIPTION']\n",
    "    for col in text_cols:\n",
    "        if col in df.columns:\n",
    "            df[col] = df[col].fillna('').astype(str).str.strip()\n",
    "            df[col] = df[col].replace(['nan', 'NaN', 'None', ''], np.nan)\n",
    "    \n",
    "    # Clean ME label\n",
    "    if 'ME_LABEL' in df.columns:\n",
    "        df['ME_LABEL'] = df['ME_LABEL'].fillna('').astype(str).str.strip().str.upper()\n",
    "        df['ME_LABEL'] = df['ME_LABEL'].replace(['NAN', ''], np.nan)\n",
    "    \n",
    "    # Clean numeric/date\n",
    "    if 'BASE_AMOUNT' in df.columns:\n",
    "        df['BASE_AMOUNT'] = pd.to_numeric(df['BASE_AMOUNT'], errors='coerce')\n",
    "    if 'JOURNAL_DATE' in df.columns:\n",
    "        df['JOURNAL_DATE'] = pd.to_datetime(df['JOURNAL_DATE'], errors='coerce')\n",
    "    \n",
    "    # Dedupe\n",
    "    dedup = [c for c in ['BUSINESS_UNIT_CODE', 'ACCOUNT_CODE', 'BASE_AMOUNT', 'JOURNAL_DATE'] if c in df.columns]\n",
    "    if dedup:\n",
    "        df = df.drop_duplicates(subset=dedup, keep='first')\n",
    "    \n",
    "    return df\n",
    "\n",
    "raw_clean = clean_data(raw_data)\n",
    "labeled_clean = clean_data(labeled_data)\n",
    "print(f\"Raw: {len(raw_clean)}, Labeled: {len(labeled_clean)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Apply FBT Mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map to category and FBT label\n",
    "labeled_clean['CATEGORY'] = labeled_clean.apply(\n",
    "    lambda r: map_to_category(r.get('TAX_DESCRIPTION'), r.get('ME_LABEL')), axis=1\n",
    ")\n",
    "labeled_clean['FBT_LABEL'] = labeled_clean['CATEGORY'].apply(get_fbt_label)\n",
    "\n",
    "print(\"Category Distribution:\")\n",
    "print(labeled_clean['CATEGORY'].value_counts())\n",
    "\n",
    "print(\"\\nFBT Label Distribution:\")\n",
    "print(labeled_clean['FBT_LABEL'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Category distribution\n",
    "cat_counts = labeled_clean['CATEGORY'].value_counts().head(15)\n",
    "colors = ['#e74c3c' if c in FBT_YES_CATEGORIES else '#2ecc71' if c in FBT_NO_CATEGORIES else '#95a5a6' \n",
    "          for c in cat_counts.index]\n",
    "axes[0].barh(range(len(cat_counts)), cat_counts.values, color=colors)\n",
    "axes[0].set_yticks(range(len(cat_counts)))\n",
    "axes[0].set_yticklabels(cat_counts.index)\n",
    "axes[0].set_xlabel('Count')\n",
    "axes[0].set_title('Category Distribution (Red=FBT, Green=No FBT)')\n",
    "axes[0].invert_yaxis()\n",
    "\n",
    "# FBT distribution\n",
    "fbt_counts = labeled_clean['FBT_LABEL'].value_counts()\n",
    "colors = ['#e74c3c' if x == 'Y' else '#2ecc71' if x == 'N' else '#95a5a6' for x in fbt_counts.index]\n",
    "axes[1].bar(fbt_counts.index.astype(str), fbt_counts.values, color=colors)\n",
    "axes[1].set_xlabel('FBT Label')\n",
    "axes[1].set_ylabel('Count')\n",
    "axes[1].set_title('FBT Distribution')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_combined_text(df: pd.DataFrame) -> pd.Series:\n",
    "    cols = ['PURPOSE', 'CHARGE_DESCRIPTION', 'LINE_DESCR', 'DESCRIPTION', 'INVOICE_DESCR', 'VENDOR_NAME']\n",
    "    available = [c for c in cols if c in df.columns]\n",
    "    \n",
    "    def combine(row):\n",
    "        parts = [str(row.get(c, '')).strip() for c in available \n",
    "                 if pd.notna(row.get(c)) and str(row.get(c)).strip()]\n",
    "        text = ' '.join(parts).lower()\n",
    "        return ' '.join(re.sub(r'[^a-z0-9\\s]', ' ', text).split())\n",
    "    \n",
    "    return df.apply(combine, axis=1)\n",
    "\n",
    "\n",
    "def create_features(df: pd.DataFrame, text_series: pd.Series) -> pd.DataFrame:\n",
    "    feat = pd.DataFrame(index=df.index)\n",
    "    \n",
    "    # Amount\n",
    "    if 'BASE_AMOUNT' in df.columns:\n",
    "        amt = df['BASE_AMOUNT'].fillna(0)\n",
    "        feat['amount_log'] = np.log1p(np.abs(amt))\n",
    "        feat['amount_is_negative'] = (amt < 0).astype(int)\n",
    "    \n",
    "    # Date\n",
    "    if 'JOURNAL_DATE' in df.columns:\n",
    "        dates = pd.to_datetime(df['JOURNAL_DATE'], errors='coerce')\n",
    "        feat['month'] = dates.dt.month.fillna(0).astype(int)\n",
    "        feat['day_of_week'] = dates.dt.dayofweek.fillna(0).astype(int)\n",
    "        feat['is_friday'] = (dates.dt.dayofweek == 4).astype(int)\n",
    "        feat['is_december'] = (dates.dt.month == 12).astype(int)\n",
    "    \n",
    "    # Sheet/Account type\n",
    "    if '_SHEET' in df.columns:\n",
    "        sheet_lower = df['_SHEET'].fillna('').str.lower()\n",
    "        feat['is_travel_sheet'] = sheet_lower.str.contains('travel|lodging').astype(int)\n",
    "        feat['is_entertainment_sheet'] = sheet_lower.str.contains('entertainment|meal').astype(int)\n",
    "        feat['is_training_sheet'] = sheet_lower.str.contains('training').astype(int)\n",
    "    \n",
    "    # Location features\n",
    "    loc_data = [loc_extractor.extract(t) for t in text_series]\n",
    "    feat['has_location'] = [d['has_location'] for d in loc_data]\n",
    "    feat['distance_km'] = [d['distance_km'] for d in loc_data]\n",
    "    feat['travel_hours'] = [d['travel_hours'] for d in loc_data]\n",
    "    feat['is_international'] = [d['is_international'] for d in loc_data]\n",
    "    feat['is_regional'] = [d['is_regional'] for d in loc_data]\n",
    "    feat['is_remote'] = [d['is_remote'] for d in loc_data]\n",
    "    \n",
    "    # Location info for output\n",
    "    feat['_location_name'] = [d['location_name'] for d in loc_data]\n",
    "    feat['_location_state'] = [d['location_state'] for d in loc_data]\n",
    "    \n",
    "    return feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_clean['combined_text'] = create_combined_text(labeled_clean)\n",
    "labeled_features = create_features(labeled_clean, labeled_clean['combined_text'])\n",
    "\n",
    "print(f\"Features: {[c for c in labeled_features.columns if not c.startswith('_')]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Prepare Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter to labeled data (FBT = Y or N, exclude UNKNOWN)\n",
    "mask = labeled_clean['FBT_LABEL'].isin(['Y', 'N'])\n",
    "\n",
    "df_train = labeled_clean[mask].copy()\n",
    "feat_train = labeled_features[mask].copy()\n",
    "\n",
    "X_text = df_train['combined_text'].values\n",
    "y = df_train['FBT_LABEL'].values\n",
    "\n",
    "# Encode\n",
    "fbt_encoder = LabelEncoder()\n",
    "y_encoded = fbt_encoder.fit_transform(y)\n",
    "\n",
    "print(f\"Training samples: {len(df_train)}\")\n",
    "print(f\"Classes: {fbt_encoder.classes_}\")\n",
    "print(f\"\\nFBT distribution:\")\n",
    "print(pd.Series(y).value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split\n",
    "X_text_train, X_text_test, X_feat_train, X_feat_test, y_train, y_test = train_test_split(\n",
    "    X_text, feat_train, y_encoded,\n",
    "    test_size=CONFIG['test_size'], random_state=CONFIG['random_state'], stratify=y_encoded\n",
    ")\n",
    "print(f\"Train: {len(X_text_train)}, Test: {len(X_text_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF\n",
    "tfidf = TfidfVectorizer(\n",
    "    max_features=CONFIG['max_features'], ngram_range=CONFIG['ngram_range'],\n",
    "    min_df=CONFIG['min_df'], max_df=CONFIG['max_df'], sublinear_tf=True\n",
    ")\n",
    "X_tfidf_train = tfidf.fit_transform(X_text_train)\n",
    "X_tfidf_test = tfidf.transform(X_text_test)\n",
    "\n",
    "# Numerical features\n",
    "num_cols = [c for c in X_feat_train.columns if not c.startswith('_')]\n",
    "scaler = StandardScaler()\n",
    "X_num_train = scaler.fit_transform(X_feat_train[num_cols].fillna(0))\n",
    "X_num_test = scaler.transform(X_feat_test[num_cols].fillna(0))\n",
    "\n",
    "# Combine\n",
    "X_train = sparse.hstack([X_tfidf_train, sparse.csr_matrix(X_num_train)])\n",
    "X_test = sparse.hstack([X_tfidf_test, sparse.csr_matrix(X_num_test)])\n",
    "\n",
    "print(f\"Features: {X_train.shape[1]} (TF-IDF: {X_tfidf_train.shape[1]}, Numerical: {len(num_cols)})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(max_iter=1000, random_state=42, class_weight='balanced', n_jobs=-1),\n",
    "    'Complement NB': ComplementNB(alpha=0.1),\n",
    "    'Linear SVC': CalibratedClassifierCV(LinearSVC(max_iter=2000, random_state=42, class_weight='balanced')),\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=200, max_depth=20, random_state=42, class_weight='balanced', n_jobs=-1),\n",
    "    'Gradient Boosting': GradientBoostingClassifier(n_estimators=100, max_depth=5, random_state=42),\n",
    "    'Extra Trees': ExtraTreesClassifier(n_estimators=200, max_depth=20, random_state=42, class_weight='balanced', n_jobs=-1)\n",
    "}\n",
    "\n",
    "results = []\n",
    "for name, model in models.items():\n",
    "    print(f\"Training {name}...\")\n",
    "    cv = StratifiedKFold(n_splits=CONFIG['cv_folds'], shuffle=True, random_state=42)\n",
    "    cv_f1 = cross_val_score(model, X_train, y_train, cv=cv, scoring='f1').mean()\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    test_f1 = f1_score(y_test, y_pred)\n",
    "    test_acc = accuracy_score(y_test, y_pred)\n",
    "    results.append({'Model': name, 'CV F1': round(cv_f1, 4), 'Test F1': round(test_f1, 4), 'Test Acc': round(test_acc, 4)})\n",
    "    print(f\"  CV F1: {cv_f1:.4f}, Test F1: {test_f1:.4f}, Acc: {test_acc:.4f}\")\n",
    "\n",
    "results_df = pd.DataFrame(results).sort_values('Test F1', ascending=False)\n",
    "print(\"\\nResults:\")\n",
    "print(results_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensemble\n",
    "ensemble = VotingClassifier(\n",
    "    estimators=[\n",
    "        ('lr', LogisticRegression(max_iter=1000, random_state=42, class_weight='balanced')),\n",
    "        ('nb', ComplementNB(alpha=0.1)),\n",
    "        ('svc', CalibratedClassifierCV(LinearSVC(max_iter=2000, random_state=42, class_weight='balanced'))),\n",
    "        ('rf', RandomForestClassifier(n_estimators=100, max_depth=15, random_state=42, class_weight='balanced', n_jobs=-1))\n",
    "    ],\n",
    "    voting='soft'\n",
    ")\n",
    "ensemble.fit(X_train, y_train)\n",
    "y_pred = ensemble.predict(X_test)\n",
    "\n",
    "print(f\"\\nEnsemble Results:\")\n",
    "print(f\"  Test F1: {f1_score(y_test, y_pred):.4f}\")\n",
    "print(f\"  Test Accuracy: {accuracy_score(y_test, y_pred):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred, target_names=fbt_encoder.classes_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "ConfusionMatrixDisplay.from_predictions(y_test, y_pred, display_labels=fbt_encoder.classes_,\n",
    "                                        ax=ax, cmap='Blues')\n",
    "ax.set_title('FBT Classification: Fringe Benefit Y/N')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_pkg = {\n",
    "    'model': ensemble,\n",
    "    'tfidf': tfidf,\n",
    "    'scaler': scaler,\n",
    "    'fbt_encoder': fbt_encoder,\n",
    "    'num_cols': num_cols,\n",
    "    'fbt_yes_cats': FBT_YES_CATEGORIES,\n",
    "    'fbt_no_cats': FBT_NO_CATEGORIES,\n",
    "    'category_rules': CATEGORY_RULES,\n",
    "    'config': CONFIG\n",
    "}\n",
    "joblib.dump(model_pkg, CONFIG['model_output'])\n",
    "print(f\"Saved: {CONFIG['model_output']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Inference Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_fbt(texts: List[str], model_path: str = CONFIG['model_output']) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Predict Fringe Benefit Tax classification.\n",
    "    \n",
    "    Returns: DataFrame with fbt_label, confidence, location, distance_km, travel_hours\n",
    "    \"\"\"\n",
    "    pkg = joblib.load(model_path)\n",
    "    \n",
    "    # Extract location\n",
    "    loc_info = [loc_extractor.extract(t) for t in texts]\n",
    "    \n",
    "    # Create features\n",
    "    X_tfidf = pkg['tfidf'].transform(texts)\n",
    "    feat_df = pd.DataFrame({\n",
    "        'amount_log': 0, 'amount_is_negative': 0,\n",
    "        'month': 0, 'day_of_week': 0, 'is_friday': 0, 'is_december': 0,\n",
    "        'is_travel_sheet': 0, 'is_entertainment_sheet': 0, 'is_training_sheet': 0,\n",
    "        'has_location': [d['has_location'] for d in loc_info],\n",
    "        'distance_km': [d['distance_km'] for d in loc_info],\n",
    "        'travel_hours': [d['travel_hours'] for d in loc_info],\n",
    "        'is_international': [d['is_international'] for d in loc_info],\n",
    "        'is_regional': [d['is_regional'] for d in loc_info],\n",
    "        'is_remote': [d['is_remote'] for d in loc_info]\n",
    "    })\n",
    "    \n",
    "    # Ensure all columns exist\n",
    "    for col in pkg['num_cols']:\n",
    "        if col not in feat_df.columns:\n",
    "            feat_df[col] = 0\n",
    "    \n",
    "    X_num = pkg['scaler'].transform(feat_df[pkg['num_cols']].fillna(0))\n",
    "    X = sparse.hstack([X_tfidf, sparse.csr_matrix(X_num)])\n",
    "    \n",
    "    # Predict\n",
    "    y_pred = pkg['model'].predict(X)\n",
    "    fbt_labels = pkg['fbt_encoder'].inverse_transform(y_pred)\n",
    "    \n",
    "    # Confidence\n",
    "    probs = pkg['model'].predict_proba(X)\n",
    "    confidence = probs.max(axis=1)\n",
    "    \n",
    "    return pd.DataFrame({\n",
    "        'text': texts,\n",
    "        'fbt_label': fbt_labels,\n",
    "        'confidence': confidence.round(3),\n",
    "        'location': [d['location_name'] for d in loc_info],\n",
    "        'state': [d['location_state'] for d in loc_info],\n",
    "        'distance_km': [d['distance_km'] for d in loc_info],\n",
    "        'travel_hours': [d['travel_hours'] for d in loc_info]\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "test_texts = [\n",
    "    \"Client dinner at restaurant Melbourne with 5 partners\",\n",
    "    \"Taxi to airport business trip Brisbane\",\n",
    "    \"Team building activity bowling Sydney\",\n",
    "    \"Training seminar registration fee\",\n",
    "    \"Christmas party catering staff 50 people\",\n",
    "    \"Flight to Roma client visit farm inspection\",\n",
    "    \"Travel connected to entertainment dinner Utrecht\",\n",
    "    \"Morning tea for team meeting\",\n",
    "    \"Accommodation for client dinner event Melbourne\",\n",
    "    \"Business travel meal whilst in Dubbo\"\n",
    "]\n",
    "\n",
    "predictions = predict_fbt(test_texts)\n",
    "print(\"\\nPredictions:\")\n",
    "print(predictions.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15. Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"FBT CLASSIFICATION PIPELINE\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\nGoal: Predict if expense is subject to Fringe Benefit Tax\")\n",
    "\n",
    "print(\"\\nFBT = Y (Taxable):\")\n",
    "for cat in FBT_YES_CATEGORIES:\n",
    "    print(f\"  - {cat}\")\n",
    "\n",
    "print(\"\\nFBT = N (Not Taxable):\")\n",
    "for cat in FBT_NO_CATEGORIES:\n",
    "    print(f\"  - {cat}\")\n",
    "\n",
    "print(f\"\\nTraining samples: {len(df_train)}\")\n",
    "print(f\"Features: {X_train.shape[1]}\")\n",
    "\n",
    "print(f\"\\nOutput columns:\")\n",
    "print(f\"  fbt_label, confidence, location, state, distance_km, travel_hours\")\n",
    "\n",
    "print(f\"\\nModel saved: {CONFIG['model_output']}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
